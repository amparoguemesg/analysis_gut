{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "tags": []
   },
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "tags": [
     "\"hide-input\""
    ]
   },
   "outputs": [],
   "source": [
    "import IPython\n",
    "# IPython.Application.instance().kernel.do_shutdown(True)\n",
    "\n",
    "# %matplotlib notebook \n",
    "#import mpld3\n",
    "#mpld3.enable_notebook()\n",
    "\n",
    "#%matplotlib widget\n",
    "%matplotlib ipympl\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "import pycwt\n",
    "import statistics\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn as sk\n",
    "import tkinter as tk\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn import decomposition\n",
    "from sklearn.decomposition import PCA\n",
    "from tkinter import *\n",
    "from tkinter import ttk\n",
    "from sklearn import preprocessing\n",
    "from datetime import date\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "from neurodsp.rhythm import sliding_window_matching\n",
    "from neurodsp.utils.download import load_ndsp_data\n",
    "from neurodsp.plts.rhythm import plot_swm_pattern\n",
    "from neurodsp.plts.time_series import plot_time_series\n",
    "from neurodsp.utils import set_random_seed, create_times\n",
    "# Import listed chormap\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.dates as md\n",
    "from matplotlib import colors as mcolors\n",
    "# Scipy\n",
    "from scipy import signal\n",
    "from scipy import ndimage\n",
    "# TKinter for selecting files\n",
    "from tkinter import Tk     # from tkinter import Tk for Python 3.x\n",
    "from tkinter.filedialog import askdirectory\n",
    "#Bokeh\n",
    "from bokeh.io import output_notebook\n",
    "\n",
    "# Add my module to python path\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "# Own libraries\n",
    "from processing.utils import *\n",
    "from visualization.graphics.plots import *\n",
    "from processing.filter import FIR_smooth\n",
    "from Neurogram import * # Recording, MyWavelet, MyWaveforms\n",
    "\n",
    "\n",
    "import matplotlib.ticker as ticker\n",
    "from scipy.signal import savgol_filter\n",
    "from matplotlib.ticker import NullFormatter\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "# Define a custom function to format the tick labels as empty strings\n",
    "def hide_tick_labels(value, pos):\n",
    "    return \"\"\n",
    "\n",
    "# Exponential Moving Average (EMA) smoothing\n",
    "def exponential_moving_average(data, alpha):\n",
    "    ema = [data[0]]\n",
    "    for i in range(1, len(data)):\n",
    "        ema.append(alpha * data[i] + (1 - alpha) * ema[-1])\n",
    "    return np.array(ema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "time_start_overall = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Tk().withdraw()  # keep the root window from appearing\n",
    "dir_name = ('../datasets/')\n",
    "#path = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## PKL load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# When using port A: channels=range(0,32,1) by default port B:range(32,64,1)\n",
    "# Start and dur in samples\n",
    "# feinstein: channels=[0]\n",
    "time_start = time.time()\n",
    "load_from_file=True\n",
    "downsample = 1          \n",
    "start=1*60*10000                    # SC chronic: 7*30000              #2*60*10000 # 0*30000 ## start=2*60*10000\n",
    "dur= None                # SC chronic: 4*30000           # EMG: 35*30000\n",
    "port = 'Port A'\n",
    "#selected_ch =[7]\n",
    "load_raw = True\n",
    "\n",
    "if load_raw:\n",
    "    record = Recording.open_record(path, start=start, dur=dur, \n",
    "                                   load_from_file=load_from_file, \n",
    "                                   load_multiple_files=True,\n",
    "                                   downsample=downsample,\n",
    "                                   port=port  ,  # Select recording port\n",
    "                                   #selected_ch =selected_ch,\n",
    "                                   map_path=map_path,\n",
    "                                   verbose=0)\n",
    "else:\n",
    "    filepath = askopenfile(initialdir=path, title=\"Select previously stored data file\", \n",
    "                                filetypes=[(\"recording\", \".csv .pkl, .parquet\")])\n",
    "    record = pd.read_parquet(filepath.name) # 'read_pickle'\n",
    "    print(record.recording)\n",
    "    record.recording.name = 'Raw' #'HF_filtered'\n",
    "\n",
    "    \n",
    "# Create directory to save figures\n",
    "if not os.path.exists('%s/figures/' %(path)):\n",
    "    os.makedirs('%s/figures/' %(path))\n",
    "print(\"Time elapsed: {} seconds\".format(time.time()-time_start)) \n",
    "\n",
    "\n",
    "# Remove NaN values \n",
    "record.recording = record.recording.dropna()\n",
    "record.recording.name = 'original'\n",
    "#sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record.channels = [7]  # ['all']\n",
    "# D4-6: ch1\n",
    "# D7: ch 10\n",
    "#rat chronic 1: [0,1,6,7,8,10,15,16,18,23,24,25,26,27,28,29,30,31] \n",
    "#rat chronic 2:[9,10,12,13,18,25,26,27,28,29,30,31]   \n",
    "#rat chronic 3: [0,1,8,10,11,25,27,28,29,30,31]\n",
    "#rat chronic 4: [1,2,3,5,25,28,29,30,31]\n",
    "# mouse_Distension: [1,4,5,6,8,10,11,18,19,23,24,25,26,27,29]\n",
    "# pig drugs = [0,1, 4,5,7, 8,9,  23, 24, 25,26, 27]  4\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get current time for saving (avoid overwriting)\n",
    "now = datetime.datetime.now()\n",
    "current_time = now.strftime(\"%d%m%Y_%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  General configuration  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": []
   },
   "source": [
    "#### Options list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "options_filter = [\n",
    "    \"None\", \n",
    "    \"butter\", \n",
    "    \"fir\"]                # Binomial Weighted Average Filter\n",
    "\n",
    "options_detection = [\n",
    "    \"get_spikes_threshCrossing\", # Ojo: get_spikes_threshCrossing needs detects also cardiac \n",
    "                                     # spikes, so use cardiac_window. This method is slower\n",
    "    \"get_spikes_method\",         # Python implemented get_spikes() method. Faster\n",
    "    \"so_cfar\"]                    # Smallest of constant false-alarm rate filter\n",
    "\n",
    "options_threshold = [\n",
    "    \"positive\",\n",
    "    \"negative\", \n",
    "    \"both_thresh\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configure\n",
    "\n",
    "config_text = []\n",
    "record.apply_filter = options_filter[1]    \n",
    "record.detect_method = options_detection[2]                                    \n",
    "record.thresh_type = options_threshold[0]\n",
    "record.path = path  \n",
    "\n",
    "config_text = ['Load_from_file %s' %load_from_file, 'Filter: %s'%record.apply_filter, 'Detection: %s'%record.detect_method, 'Threhold type: %s'%record.thresh_type, 'Channels: %s' %record.channels, 'Downsampling: %s' %downsample]\n",
    "config_text.append('Port %s' %(port))\n",
    "config_text.append('Start %s, Dur: %s' %(start,dur))\n",
    "config_text.append('Channels: %s' %record.channels)\n",
    "# Ramarkable timestamps (in sec) \n",
    "\n",
    "group = ''\n",
    "\n",
    "print('SELECTED GENERAL CONFIGURATION:')\n",
    "print('Filter: %s'%record.apply_filter)\n",
    "print('Detection: %s'%record.detect_method)\n",
    "print('Threhold type: %s'%record.thresh_type)\n",
    "print('Channels: %s' %record.channels) \n",
    "print('-------------------------------------')\n",
    "\n",
    "record.select_channels(record.channels) # keep_ch_loc=True if we want to display following the map. Otherwise follow the order provided by selected channels.\n",
    "print('map_array: %s' %record.map_array)\n",
    "print('ch_loc: %s' %record.ch_loc)\n",
    "print('filter_ch %s' %record.filter_ch)\n",
    "#print('column_ch %s' %record.column_ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "time_marks = []# [30, 40, 46, 50] #[15, 18.1, 18.8, 20, 23, 25, 28, 32, 34]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select visualization options:  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configure\n",
    "record.num_rows = 1+ int(round(len(record.filter_ch)/4)) # round(n_components/2) #8    # 1  int(len(record.filter_ch))#\n",
    "record.num_columns = 4#int(len(record.filter_ch)-round(len(record.filter_ch)/2)) # 2 #4     # 1\n",
    "plot_ch = int(record.map_array[record.ch_loc[0]-1])                 # int(record.filter_ch[0][-2:])  # Get first channel to visualise\n",
    "print(plot_ch)\n",
    "print(record.num_rows)\n",
    "print(record.num_columns)\n",
    "save_figure = True\n",
    "config_text.append('Visualization options: %sx%s, topo_plot: %s, save_figure: %s' %(record.num_rows, record.num_columns, topo_plot, save_figure))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## START ANALYSIS                                             \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot raw signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "colorbar_ticks_raw=[20, 0, -30] #, -100, -500]#, -150]#, -200, -250]\n",
    "\n",
    "cmap = 'gist_ncar' # 'nipy_spectral'\n",
    "\n",
    "record.plot_freq_content(record.original,int(plot_ch), nperseg=512, max_freq=2000, ylim=[-1500,1500], dtformat='%H:%M:%S',\n",
    "                         figsize=(10, 10), savefigpath='%s/figures/%s_ch%s_original-%s.tiff' %(record.path, port, plot_ch, current_time), \n",
    "                         show=True,  cmap=cmap, colorbar_ticks=colorbar_ticks_raw) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Channel referencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "channels =  record.filter_ch #median_channels  #record.filter_ch\n",
    "ref_ch_name = 'mean' #'mean'\n",
    "if ref_ch_name == 'mean':\n",
    "    #all_ch_list = [col for col in record.original.columns if col.startswith('ch_')]\n",
    "    all_ch_list = [col for col in channels if col.startswith('ch_')] \n",
    "    ref_ch = record.original[all_ch_list].mean(axis=1)\n",
    "else:\n",
    "    ref_ch = record.original['ch_%s'%ref_ch_name]  \n",
    "record.referenced = record.original[record.filter_ch].sub(ref_ch, axis=0)\n",
    "record.referenced['seconds'] = record.original['seconds']\n",
    "record.recording=record.referenced\n",
    "record.recording.name = 'referenced'\n",
    "config_text.append('ref_ch: %s' % ref_ch_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute power in frequency bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.signal import welch\n",
    "\n",
    "# All time analysis (not time evolution)\n",
    "\n",
    "# Define frequency bands\n",
    "bands = {\n",
    "    \"0-0.05Hz\": [0, 0.05],\n",
    "    \"0.05-0.2Hz\": [0.05, 0.2],\n",
    "    \"0.2-1Hz\": [0.2, 1], #0.4 to 0.7 circular smooth muscle\n",
    "    \"1Hz-5Hz\": [1, 5], \n",
    "    \"5Hz-300Hz\": [5, 300],\n",
    "    #\"5Hz-10Hz\": [5, 10],\n",
    "    #\"10Hz-20Hz\": [10, 20],\n",
    "    #\"20Hz-300Hz\": [20, 300],\n",
    "    #\"100-200Hz\": [100, 200],\n",
    "    #\"200-300Hz\": [200, 300],\n",
    "    \"300-2000Hz\": [300, 2000],\n",
    "}\n",
    "\n",
    "# Bandpass filter design\n",
    "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
    "    nyquist = 0.5 * fs\n",
    "    low = lowcut / nyquist\n",
    "    high = highcut / nyquist\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return b, a\n",
    "\n",
    "# Apply bandpass filter to the data\n",
    "def bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
    "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "    y = filtfilt(b, a, data)\n",
    "    return y\n",
    "\n",
    "# Function to compute Power Spectral Density (PSD) using Welch's method\n",
    "def compute_psd(data, fs, nperseg, noverlap):\n",
    "    freqs, psd = welch(data, fs, nperseg=nperseg, noverlap=noverlap)\n",
    "    return freqs, psd\n",
    "\n",
    "# Function to plot PSD\n",
    "def plot_psd(freqs, psd, ch, xlabel=\"Frequency (Hz)\", ylabel=\"Power/Frequency (dB/Hz)\"):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.semilogy(freqs, psd)\n",
    "    plt.title(\"Power Spectral Density - ch_%s\"%ch)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "# Function to compute power in a given frequency band\n",
    "def band_power(psd, freqs, band):\n",
    "    band_freqs = np.logical_and(freqs >= band[0], freqs <= band[1])\n",
    "    return np.trapz(psd[band_freqs], freqs[band_freqs])\n",
    "\n",
    "# Load your DataFrame here\n",
    "df = record.recording  # Load your DataFrame here\n",
    "fs = record.fs  # Sampling rate in Hz\n",
    "\n",
    "# Calculate band widths\n",
    "band_widths = {band: bands[band][1] - bands[band][0] for band in bands}\n",
    "\n",
    "# Initialize a dictionary to hold power results\n",
    "results = {}\n",
    "\n",
    "# Parameters for Welch's method\n",
    "nperseg = 1000000  # Segment length for 0.01 Hz resolution\n",
    "noverlap = nperseg // 2  # 50% overlap\n",
    "\n",
    "# Iterate over each channel in the DataFrame\n",
    "for channel in df.columns:\n",
    "    # Compute PSD for the current channel\n",
    "    freqs, psd = compute_psd(df[channel], fs, nperseg=nperseg, noverlap=noverlap)\n",
    "    \n",
    "    # Calculate power for each band for the current channel\n",
    "    band_powers = {band: band_power(psd, freqs, bands[band]) for band in bands}\n",
    "    \n",
    "    # Normalize by band width By dividing the band power by the band width to make comparisons fair.\n",
    "    band_powers_normalized = {band: band_powers[band] / band_widths[band] for band in bands}\n",
    "    \n",
    "    # Store the results\n",
    "    results[channel] = {\n",
    "        **band_powers,\n",
    "        **{f\"{band}_normalized\": band_powers_normalized[band] for band in bands}\n",
    "    }\n",
    "\n",
    "# Create a DataFrame to store the results for easier visualization\n",
    "results_df = pd.DataFrame(results).T\n",
    "\n",
    "# Calculate total power for normalization: Additional normalization by the total power for dimensionless comparison.\n",
    "total_power = results_df[list(bands.keys())].sum(axis=1)\n",
    "\n",
    "# Normalize power for each band by total power\n",
    "for band in bands:\n",
    "    results_df[f'{band}_total_normalized'] = results_df[band] / total_power\n",
    "\n",
    "# Save the results to a text file\n",
    "output_file_path = '%s/figures/%s_power_results_%s_%s.csv'%(record.path, port, record.recording.name, current_time)\n",
    "results_df.to_csv(output_file_path)\n",
    "\n",
    "# Save the results to a pkl file\n",
    "output_file_path = '%s/figures/%s_power_results_%s_%s.pkl'%(record.path, port, record.recording.name, current_time)\n",
    "with open(output_file_path, 'wb') as f:\n",
    "    pickle.dump(results_df, f)\n",
    "\n",
    "# Print results with units\n",
    "#print(\"Results (units: V^2 for absolute power, dimensionless for normalized power):\")\n",
    "#print(results_df)\n",
    "\n",
    "#-------------------------------------------\n",
    "# Determine the number of rows needed\n",
    "num_channels = len(df.columns)\n",
    "num_cols = 4\n",
    "num_rows = int(np.ceil(num_channels / num_cols))\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(22, num_rows * 3))\n",
    "\n",
    "# Flatten axes for easy indexing if more than one subplot\n",
    "if num_channels > 1:\n",
    "    axes = axes.flatten()\n",
    "\n",
    "# Plot each channel\n",
    "for i, channel in enumerate(df.columns[0:-1]):\n",
    "    ax = axes[i]  # Get the correct subplot\n",
    "    results_df.loc[channel, [f'{band}_total_normalized' for band in bands]].plot(kind='bar', ax=ax, color='green')\n",
    "    ax.set_title(f'Normalized Power in {channel}')\n",
    "    ax.set_ylabel('Normalized Power (unitless)')\n",
    "    ax.set_xlabel('Frequency Bands')\n",
    "\n",
    "# Hide any unused subplots\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "plt.savefig('%s/figures/%s_total_norm_power_per ch_%s-%s.png' %(record.path, port, record.recording.name, current_time), facecolor='w')\n",
    "\n",
    "#-----------------------------------------------\n",
    "# Plot the results for visualization\n",
    "fig, axes = plt.subplots(nrows=2, ncols=len(bands), figsize=(18, 12))\n",
    "\n",
    "# Top row: Normalized Power\n",
    "for i, band in enumerate(bands):\n",
    "    results_df[[f'{band}_total_normalized']].plot(kind='bar', ax=axes[0, i], color='green')\n",
    "    axes[0, i].set_title(f'Normalized Power in {band}')\n",
    "    axes[0, i].set_ylabel('Normalized Power (unitless)')\n",
    "    axes[0, i].set_ylim([0,1])\n",
    "\n",
    "# Bottom row: Absolute Power\n",
    "for i, band in enumerate(bands):\n",
    "    results_df[[band]].plot(kind='bar', ax=axes[1, i], color='blue')\n",
    "    axes[1, i].set_title(f'Absolute Power in {band}')\n",
    "    axes[1, i].set_ylabel('Power (V^2)')\n",
    "    axes[1, i].set_ylim([0,150000])\n",
    "    \n",
    "plt.savefig('%s/figures/%s_power_allCH_per band_%s-%s.png' %(record.path, port, record.recording.name, current_time), facecolor='w')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "#sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evolution over time\n",
    "\n",
    "### Meal times\n",
    "meal_r1_d1 = pd.Timestamp('1970-01-01 00:15:15')  #  pd.Timestamp('1970-01-01 00:28:20')   Update with your actual meal time\n",
    "meal_r2_d1 = pd.Timestamp('1970-01-01 00:15:45')\n",
    "meal_r3_d1 = pd.Timestamp('1970-01-01 00:15:48')\n",
    "meal_r4_d1 = pd.Timestamp('1970-01-01 00:20:10')\n",
    "\n",
    "meal_r1_d8 = pd.Timestamp('1970-01-01 00:15:05 ')   #up to 16:15 #14:40? Rat 1, defecation\n",
    "meal_r2_d8 = pd.Timestamp('1970-01-01 00:15:17 ') \n",
    "meal_r3_d8 = pd.Timestamp('1970-01-01 00:15:05 ')\n",
    "meal_r4_d8 = pd.Timestamp('1970-01-01 00:15:47 ')\n",
    "\n",
    "meal_r1_d12 = [pd.Timestamp('1970-01-01 00:15:09'), pd.Timestamp('1970-01-01 00:21:32')]\n",
    "meal_r2_d12 = [pd.Timestamp('1970-01-01 00:20:09'), pd.Timestamp('1970-01-01 00:28:07')]\n",
    "meal_r3_d12 = [pd.Timestamp('1970-01-01 00:15:23'), pd.Timestamp('1970-01-01 00:15:58')]\n",
    "meal_r4_d12 = pd.Timestamp('1970-01-01 00:15:21')\n",
    "meal_time = pd.Timestamp('1970-01-01 00:00:00') # meal_r3_d8\n",
    "\n",
    "# Parameters for Welch's method\n",
    "nperseg = 1000000  # Segment length for 0.01 Hz resolution\n",
    "noverlap = 0#nperseg // 2  # 50% overlap\n",
    "\n",
    "interv = '1min'\n",
    "\n",
    "# Define the intervals (example times, adjust as needed)\n",
    "intervals = {\n",
    "    '1min': pd.Timedelta(minutes=1),\n",
    "    '2min': pd.Timedelta(minutes=2),\n",
    "    '5min': pd.Timedelta(minutes=5)\n",
    "}\n",
    "\n",
    "# Define the recording start and end times\n",
    "recording_start = record.recording.index[0]\n",
    "recording_end = record.recording.index[-1]\n",
    "\n",
    "# Create intervals for the entire recording\n",
    "num_intervals = (recording_end - recording_start) // intervals[interv]\n",
    "interval_times = [recording_start + i * intervals[interv] for i in range(num_intervals)]\n",
    "interval_times.append(recording_end)  # Include the end time\n",
    "\n",
    "\n",
    "# Create DataFrame splits for each interval\n",
    "df_intervals = {interval_start: df.loc[(df.index >= interval_start) & (df.index < interval_end)] \n",
    "                 for interval_start, interval_end in zip(interval_times[:-1], interval_times[1:])}\n",
    "\n",
    "# Initialize dictionaries to store results\n",
    "results_intervals = {interval_start: {} for interval_start in df_intervals.keys()}\n",
    "# Initialize dictionary to store average results\n",
    "average_results = {band: [] for band in bands}\n",
    "\n",
    "# Function to process and store results for a given interval\n",
    "def process_band_power(df_part, results_dict):\n",
    "    for channel in df_part.columns:\n",
    "        freqs, psd = compute_psd(df_part[channel], fs, nperseg=nperseg, noverlap=noverlap)\n",
    "        band_powers = {band: band_power(psd, freqs, bands[band]) for band in bands}\n",
    "        band_widths = {band: bands[band][1] - bands[band][0] for band in bands}\n",
    "        band_powers_normalized = {band: band_powers[band] / band_widths[band] for band in bands}\n",
    "        results_dict[channel] = {\n",
    "            **band_powers,\n",
    "            **{f\"{band}_normalized\": band_powers_normalized[band] for band in bands}\n",
    "        }\n",
    "\n",
    "# Process each interval\n",
    "for interval_start, df_part in df_intervals.items():\n",
    "    process_band_power(df_part, results_intervals[interval_start])\n",
    "\n",
    "# Convert results to DataFrames\n",
    "results_dfs = {start: pd.DataFrame(results_intervals[start]).T for start in results_intervals}\n",
    "\n",
    "# Calculate total power for normalization\n",
    "total_powers = {start: results_dfs[start][list(bands.keys())].sum(axis=1) for start in results_dfs}\n",
    "\n",
    "# Normalize power for each band by total power\n",
    "for start in results_dfs:\n",
    "    for band in bands:\n",
    "        results_dfs[start][f'{band}_total_normalized'] = results_dfs[start][band] / total_powers[start]\n",
    "\n",
    "# Save the results to a text file\n",
    "output_file_path = '%s/figures/%s_power_results_%sinterv_%s_%s.pkl'%(record.path, port, interv, record.recording.name, current_time)\n",
    "with open(output_file_path, 'wb') as f:\n",
    "    pickle.dump(results_dfs, f)\n",
    "\n",
    "#-----------------------------------------------------------------\n",
    "#### Normalised per band width and total power\n",
    "# Define the colormap\n",
    "import seaborn as sns\n",
    "colormap = sns.color_palette(\"cividis\", as_cmap=True)\n",
    "#colormap = get_map('cividis')\n",
    "\n",
    "# Create subplots\n",
    "num_channels = len(df.columns)\n",
    "num_cols = 2\n",
    "num_rows = int(np.ceil(num_channels / num_cols))\n",
    "\n",
    "fig, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(15, num_rows * 5))\n",
    "\n",
    "# Flatten axes for easy indexing if more than one subplot\n",
    "if num_channels > 1:\n",
    "    axes = axes.flatten()\n",
    "\n",
    "# Prepare data for plotting\n",
    "for i, channel in enumerate(df.columns):\n",
    "    ax = axes[i]  # Get the correct subplot\n",
    "    df_combined = pd.DataFrame({\n",
    "        band: [\n",
    "            results_dfs[interval_start].loc[channel, f'{band}_total_normalized']\n",
    "            for interval_start in df_intervals\n",
    "        ]\n",
    "        for band in bands\n",
    "    }).T\n",
    "\n",
    "   \n",
    "    df_combined.columns = [f'Interval {j+1} (%s)'%interv for j in range(len(df_combined.columns))]\n",
    "    df_combined.plot(kind='bar', ax=ax, color=colormap(np.linspace(0, 1, len(df_combined.columns))), legend=False) # plt.cm.viridis(np.linspace(0, 1, len(df_combined.columns)))\n",
    "    #ax.axvline(x=meal_time, color='red', linestyle='--', linewidth=2, label='Vertical Line')\n",
    "\n",
    "    ax.set_title(f'Normalized Power in {channel}')\n",
    "    ax.set_ylabel('Normalized Power (unitless)')\n",
    "    ax.set_xlabel('Power Bands')\n",
    "    ax.set_xticks(np.arange(len(bands)))\n",
    "    ax.set_xticklabels(bands.keys(), rotation=45, ha='right')\n",
    "    ax.set_ylim([0,1])\n",
    "    \n",
    "    # Show legend only in the last subplot\n",
    "    if (i == len(df.columns) - 1) and interv=='5min':\n",
    "        ax.legend(title='Time Points')\n",
    "\n",
    "# Hide any unused subplots\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('%s/figures/%s_power_per_ch_evol_%s_%s-%s.tiff' % (record.path, port, interv, record.recording.name, current_time), facecolor='w')\n",
    "plt.show()\n",
    "\n",
    "# Compute the average power for each band across all channels\n",
    "avg_results = {interval_start: results_dfs[interval_start].mean(axis=0) for interval_start in results_dfs}\n",
    "\n",
    "# Create subplots for the average\n",
    "fig_avg, ax_avg = plt.subplots(figsize=(12, 15))\n",
    "\n",
    "# Prepare data for plotting\n",
    "df_avg_combined = pd.DataFrame({\n",
    "    band: [avg_results[interval_start][f'{band}_total_normalized'] for interval_start in df_intervals]\n",
    "    for band in bands\n",
    "}).T\n",
    "df_avg_combined.index = [band for band in bands]\n",
    "\n",
    "# Plot average power across all channels\n",
    "df_avg_combined.plot(kind='bar', ax=ax_avg, color=plt.cm.viridis(np.linspace(0, 1, len(df_avg_combined.columns))))\n",
    "ax_avg.set_title('Average Normalized Power Across All Channels')\n",
    "ax_avg.set_ylabel('Normalized Power (unitless)')\n",
    "ax_avg.set_xlabel('Power Bands')\n",
    "ax_avg.set_xticklabels(bands.keys(), rotation=45)\n",
    "ax_avg.set_ylim([0,1])\n",
    "ax_avg.legend([f'{interval_start}' for interval_start in df_intervals])\n",
    "#ax_avg.axvline(x=meal_time, color='red', linestyle='--', linewidth=2, label='Vertical Line')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('%s/figures/%s_average_power_total_norm_evol_%s_%s-%s.tiff' % (record.path, port, interv, record.recording.name, current_time), facecolor='w')\n",
    "plt.show()\n",
    "\n",
    " # Save the results to a text file\n",
    "output_file_path = '%s/figures/%s_average_power_total_norm_evol_%s_%s_%s.csv'%(record.path, port, record.recording.name, interv, current_time)\n",
    "df_avg_combined.to_csv(output_file_path)\n",
    "    \n",
    "\n",
    "#sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Normalised per band width, not per total power\n",
    "## Per channel\n",
    "# Define the colormap\n",
    "import seaborn as sns\n",
    "colormap = sns.color_palette(\"cividis\", as_cmap=True)\n",
    "\n",
    "# Create subplots\n",
    "num_channels = len(df.columns)\n",
    "num_cols = 2\n",
    "num_rows = int(np.ceil(num_channels / num_cols))\n",
    "\n",
    "fig, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(20, num_rows * 5))\n",
    "\n",
    "# Flatten axes for easy indexing if more than one subplot\n",
    "if num_channels > 1:\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "# Prepare data for plotting\n",
    "for i, channel in enumerate(df.columns):\n",
    "    ax = axes[i]  # Get the correct subplot\n",
    "    df_combined = pd.DataFrame({\n",
    "        band: [\n",
    "            results_dfs[interval_start].loc[channel, f'{band}_normalized']\n",
    "            for interval_start in df_intervals\n",
    "        ]\n",
    "        for band in bands\n",
    "    }).T\n",
    "   \n",
    "    df_combined.columns = [f'Interval {j+1} (%s)'%interv for j in range(len(df_combined.columns))]\n",
    "    df_combined.plot(kind='bar', ax=ax, color=colormap(np.linspace(0, 1, len(df_combined.columns))), legend=False) # plt.cm.viridis(np.linspace(0, 1, len(df_combined.columns)))\n",
    "    #ax.axvline(x=meal_time, color='red', linestyle='--', linewidth=2, label='Vertical Line')\n",
    "\n",
    "    ax.set_title(f'Normalized Power in {channel}')\n",
    "    ax.set_ylabel('Normalized Power (unitless)')\n",
    "    ax.set_xlabel('Power Bands')\n",
    "    ax.set_xticks(np.arange(len(bands)))\n",
    "    ax.set_xticklabels(bands.keys(), rotation=45, ha='right')\n",
    "    #ax.set_ylim([0,1])\n",
    "    \n",
    "    # Show legend only in the last subplot\n",
    "    if (i == len(df.columns) - 1) and interv=='5min':\n",
    "        ax.legend(title='Time Points')\n",
    "\n",
    "# Hide any unused subplots\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('%s/figures/%s_power_norm_width_per_ch_evol_%s_%s-%s.tiff' % (record.path, port, interv, record.recording.name, current_time), facecolor='w')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "## Average channels\n",
    "# Compute the average absolute power for each band across all channels and intervals\n",
    "avg_abs_results = {interval_start: results_dfs[interval_start][list(bands.keys())].mean(axis=0) for interval_start in results_dfs}\n",
    "\n",
    "# Create subplots for the average absolute power\n",
    "fig_avg_abs, ax_avg_abs = plt.subplots(figsize=(12, 15))\n",
    "\n",
    "# Prepare data for plotting\n",
    "df_avg_abs_combined = pd.DataFrame({\n",
    "    band: [avg_results[interval_start][f'{band}_normalized'] for interval_start in df_intervals]\n",
    "    for band in bands\n",
    "}).T\n",
    "df_avg_abs_combined.index = [band for band in bands]\n",
    "\n",
    "# Plot average absolute power across all channels\n",
    "df_avg_abs_combined.plot(kind='bar', ax=ax_avg_abs, color=plt.cm.plasma(np.linspace(0, 1, len(df_avg_abs_combined.columns))))\n",
    "ax_avg_abs.set_title('Average Absolute Power (norm per width) Across All Channels')\n",
    "ax_avg_abs.set_ylabel('Absolute Power')\n",
    "ax_avg_abs.set_xlabel('Power Bands')\n",
    "ax_avg_abs.set_xticklabels(bands.keys(), rotation=45)\n",
    "ax_avg_abs.set_ylim([0, df_avg_abs_combined.values.max() * 1.1])\n",
    "ax_avg_abs.legend([f'{interval_start}' for interval_start in df_intervals])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('%s/figures/%s_average_power_norm_width_evol_%s_interv_%s_%s.tiff' % (record.path, port, interv, record.recording.name, current_time), facecolor='w')\n",
    "plt.show()\n",
    " # Save the results to a text file\n",
    "output_file_path = '%s/figures/%s_average_power_norm_width_evol_%s_interv_%s_%s.csv'%(record.path, port, record.recording.name, interv, current_time)\n",
    "df_avg_abs_combined.to_csv(output_file_path)\n",
    "\n",
    "#sys.exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Filtering\n",
    "*Done: optimised filtering so that it uses second-order-cascade (sos). See method filter() in Neurogram.py for more details*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bandwidth filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Configure\n",
    "filt_config = {\n",
    "    'W': [0.01, 2000], # [300,2000], #,4000], #drugs HF[300,2000],# Chronic:[300,4000], #[300], #  needs to be <fs/2 per Nyquist33\n",
    "    'None': {},\n",
    "    'butter': {\n",
    "            'N': 9,                # The order of the filter\n",
    "            'btype': 'bandpass' #'bandpass' #'bandpass', #'bandpass', #'hp'  #'lowpass'     # The type of filter.\n",
    "    },      \n",
    "    'fir': {\n",
    "            'n': 4,\n",
    "    },\n",
    "    'notch': {\n",
    "            'quality_factor': 30,\n",
    "    },\n",
    "}\n",
    "\n",
    "#filt_config['butter']['Wn'] = fnorm(filt_config['W'], fs=record.fs).tolist() # The critical frequency or frequencies.\n",
    "# Same as doing (filt_config['W']/(record.fs/2)).tolist()\n",
    "filt_config['butter']['Wn'] = filt_config['W']\n",
    "filt_config['butter']['fs'] = record.fs\n",
    "\n",
    "config_text.append('filt_config: %s' %json.dumps(filt_config))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Apply filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if load_raw:\n",
    "\n",
    "    # Configure\n",
    "    time_start = time.time()\n",
    "    signal2filter = record.recording\n",
    "    config_text.append('signal2filter: %s' %signal2filter.name)\n",
    "    record.filter(signal2filter, record.apply_filter, **filt_config[record.apply_filter])\n",
    "    # Change from float64 to float 16\n",
    "    record.filtered = convertDfType(record.filtered, typeFloat='float32')\n",
    "    #print(record.filtered.dtypes)\n",
    "    print(\"Time elapsed: {} seconds\".format(time.time()-time_start))\n",
    "    \n",
    "    record.recording=record.filtered\n",
    "    record.recording.name = 'filtered'\n",
    "    #print('Saving data into: %s/0-2000Hz_filtered_30Khz_ch%s' %(record.path, plot_ch))\n",
    "    #record.filtered.to_parquet(r'%s/0-2000Hz_filtered_30Khz_ch%s.parquet' %(record.path, plot_ch), index=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Plot filtered signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "no_label = False\n",
    "\n",
    "current_time = now.strftime(\"%d%m%Y_%H%M%S\")\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['agg.path.chunksize'] = 10000\n",
    "\n",
    "cmap = 'gist_ncar' # 'nipy_spectral'\n",
    "\n",
    "freq_max = 6000 #chronic: 4000\n",
    "textf = 'HF'\n",
    "colorbar_ticks_filt= [0, -150]#, -30 -50]#, -100, -150, -200] # in vivo: [5, 0, -50, -100, -150, -200]#, -250] #[-10, -35]  ex vivo: [-50, -100]\n",
    "nperseg=512\n",
    "\n",
    "#%matplotlib notebook \n",
    "text_label = 'Filtered'\n",
    "text = 'Channels after %s filtering'%record.apply_filter\n",
    "\n",
    "record.plot_freq_content(record.filtered,int(plot_ch), nperseg=nperseg, max_freq=freq_max, ylim=[-500,500], dtformat='%H:%M:%S', #ex vivo =[-0.5,0.5] In vivo: -120,120\n",
    "                         figsize=(10, 10), savefigpath='%s/figures/%s_ch%s_%s_lab%s-%s.svg' %(record.path,port,plot_ch, textf, no_label, current_time),\n",
    "                         show=True, cmap=cmap, colorbar_ticks=colorbar_ticks_filt, no_label=no_label) \n",
    "sys.exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notch filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# for drugs\n",
    "if load_raw:\n",
    "    time_start = time.time()\n",
    "    freq_notch = range(50,2000, 100) # 250, 2000, 100 range(350, 5000, 100) #2000 [1650] #[350, 450, 550, 1550]\n",
    "    for n in freq_notch:\n",
    "        filt_config['notch']['notch_freq'] = n\n",
    "        record.filter(record.filtered, 'notch', **filt_config['notch'])\n",
    "    print(\"Time elapsed: {} seconds\".format(time.time()-time_start))\n",
    "    config_text.append('notch_filtered:applied')\n",
    "    # Change from float64 to float 16\n",
    "    record.filtered = convertDfType(record.filtered, typeFloat='float32')\n",
    "\n",
    "    record.recording=record.filtered\n",
    "    record.recording.name = 'filtered'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot filtered signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if load_raw:\n",
    "    record.path = path\n",
    "\n",
    "    ylim = [-400, 400]\n",
    "    no_label = False\n",
    "\n",
    "    text_label = 'Filtered'\n",
    "    text = 'Channels after %s filtering'%'notch'\n",
    "    colorbar_ticks= [50, -45] #[0, -50] #[15, 0, -50, -100, -150, -200] # [-20, -50]\n",
    "\n",
    "    no_label = False\n",
    "    cmap = 'gist_ncar' # 'nipy_spectral'\n",
    "    nperseg = 512\n",
    "    freq_max = 2000\n",
    "\n",
    "    record.plot_freq_content(record.recording, int(plot_ch), ylim=ylim, nperseg=nperseg, max_freq=freq_max, dtformat='%H:%M:%S',\n",
    "                             figsize=(10, 10), savefigpath='%s/figures/%s_ch%s_allfilt_lab%s-%s.svg' %(record.path, port,plot_ch,no_label, current_time), \n",
    "                             show=True, cmap=cmap, colorbar_ticks=colorbar_ticks, no_label=no_label)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Plot extrated periods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ch = record.channels[0]\n",
    "seconds_bef = 10 # 10 # drugs: 10  contraction: 30seg  distension: 0  chronic:0 \n",
    "seconds_after = 480 # 91    # drugs: 91   contraction: 160s  distension: 10  chronic:60  drugs_pig: 91\n",
    "seconds = '%s-%s'%(seconds_bef,seconds_after) #'10-91s'\n",
    "signal2plot= ''#'smoothed'\n",
    "\n",
    "'''\n",
    "if filt_config['butter']['btype'] == 'lowpass':\n",
    "    freq_max = 100\n",
    "else:\n",
    "    freq_max = filt_config['W'][1]\n",
    "'''\n",
    "freq_max =2000  \n",
    "no_label = False\n",
    "\n",
    "# Step 1: Define the multiple time points you want to use as references\n",
    "# iso 5\n",
    "#time_points =  pd.to_datetime(['1970-01-01 00:06:00', '1970-01-01 00:07:00', '1970-01-01 00:08:00', '1970-01-01 00:10:00', '1970-01-01 00:12:00']) \n",
    "#labels = ['Dist', 'Dist', 'Iso5%', 'Dist', 'Dist']\n",
    "#iso 5 5: no iso\n",
    "#time_points =  pd.to_datetime(['1970-01-01 00:06:00', '1970-01-01 00:07:00', '1970-01-01 00:10:00', '1970-01-01 00:12:00']) \n",
    "#labels = ['Dist', 'Dist', 'Dist', 'Dist']\n",
    "\n",
    "#silencing 1\n",
    "#time_points =  pd.to_datetime(['1970-01-01 00:03:00', '1970-01-01 00:05:00',  '1970-01-01 00:08:00', '1970-01-01 00:10:00']) #'1970-01-01 00:06:00',\n",
    "#labels = ['Dist', 'Dist', 'Dist', 'Dist'] #'Iso5%'\n",
    "\n",
    "#silencing 1 - drug ch7\n",
    "#time_points =  pd.to_datetime(['1970-01-01 00:02:00', '1970-01-01 00:05:00', '1970-01-01 00:10:00']) \n",
    "#labels = ['Fluid (potentially some BK)', 'Inject', 'Dist']\n",
    "#color_list = ['darkblue', 'cornflowerblue', 'darkred']\n",
    "\n",
    "#silencing 2\n",
    "#time_points =  pd.to_datetime(['1970-01-01 00:03:00', '1970-01-01 00:05:00', '1970-01-01 00:08:00', '1970-01-01 00:10:00', '1970-01-01 00:15:45'])  # '1970-01-01 00:06:00', '1970-01-01 00:10:26', '1970-01-01 00:15:45'\n",
    "#labels = ['Dist', 'Dist',  'Dist', 'Dist', 'Dist'] # 'Iso5%','Iso1%', 'Dist'\n",
    "#colors = ['darkblue', 'cornflowerblue', 'darkred', 'indianred', 'cornflowerblue']\n",
    "\n",
    "#silencing 2 - bath\n",
    "#time_points =  pd.to_datetime(['1970-01-01 00:00:01', '1970-01-01 00:01:00', '1970-01-01 00:03:00', '1970-01-01 00:04:00']) \n",
    "#labels = ['Before', 'BK', 'in between', 'Capsaicing serosa']\n",
    "#color_list = ['black','darkblue', 'black', 'darkred']\n",
    "#time_points =  pd.to_datetime(['1970-01-01 00:00:01', '1970-01-01 00:01:00', '1970-01-01 00:04:00']) \n",
    "#labels = ['Before', 'BK', 'Capsaicing serosa']\n",
    "\n",
    "#silencing 2 - drug\n",
    "#time_points =  pd.to_datetime(['1970-01-01 00:02:00', '1970-01-01 00:05:00', '1970-01-01 00:10:00', '1970-01-01 00:12:00']) \n",
    "#labels = ['Fluid (potentially some BK)', 'Inject', 'Dist', 'Dist']\n",
    "#color_list = ['darkblue', 'cornflowerblue', 'darkred', 'indianred']\n",
    "\n",
    "\n",
    "#silencing 3\n",
    "#time_points =  pd.to_datetime(['1970-01-01 00:04:30', '1970-01-01 00:06:30',  '1970-01-01 00:11:30', '1970-01-01 00:13:30']) \n",
    "#time_points =  pd.to_datetime(['1970-01-01 00:04:30', '1970-01-01 00:06:30', '1970-01-01 00:07:30', '1970-01-01 00:11:30', '1970-01-01 00:13:30'])\n",
    "#labels =  ['Dist', 'Dist', 'Dist', 'Dist'] # ['Dist', 'Dist', 'Iso5%', 'Dist', 'Dist']\n",
    "#color_list = ['#700a97',  '#fe5803', 'darkred',  '#1a1aeb'] #'#a000c6', '#0044fb',\n",
    "\n",
    "# 14_07_2023_rat4\n",
    "#time_points =  pd.to_datetime(['1970-01-01 00:02:00', '1970-01-01 00:10:00', '1970-01-01 00:11:00', '1970-01-01 00:12:00', '1970-01-01 00:14:00', '1970-01-01 00:16:02', '1970-01-01 00:17:30',  '1970-01-01 00:19:10', '1970-01-01 00:22:00', '1970-01-01 00:25:10', '1970-01-01 00:27:06', '1970-01-01 00:31:18', '1970-01-01 00:34:10', '1970-01-01 00:38:40', '1970-01-01 00:44:00', '1970-01-01 00:46:05', '1970-01-01 00:48:00'])\n",
    "#labels = ['iso 1%','ASP', 'ASP2', 'Wash', 'BK', 'BK2', 'Wash', 'Wash', 'BK3', 'Wash', 'Wash', 'BK4','Wash', 'Wash', 'BK5', 'Wash', 'Wash', 'Capsaicin serosa','Capsaicin serosa2', 'Capsaicin injection', 'Capsaicin injection3']\n",
    "#time_points =  pd.to_datetime(['1970-01-01 00:10:00', '1970-01-01 00:11:00', '1970-01-01 00:12:00', '1970-01-01 00:14:00', '1970-01-01 00:16:02', '1970-01-01 00:17:30',  '1970-01-01 00:19:10', '1970-01-01 00:22:00', '1970-01-01 00:25:10', '1970-01-01 00:27:06', '1970-01-01 00:31:18', '1970-01-01 00:34:10', '1970-01-01 00:38:40', '1970-01-01 00:44:00', '1970-01-01 00:46:05', '1970-01-01 00:48:00'])\n",
    "#labels = ['ASP', 'ASP2', 'Wash', 'BK', 'BK2', 'Wash', 'Wash', 'BK3', 'Wash', 'Wash', 'BK4','Wash', 'Wash', 'Capsaicin serosa','Capsaicin serosa2', 'Capsaicin injection', 'Capsaicin injection3']\n",
    "#color_list = ['darkblue', 'cornflowerblue', 'black', 'darkred','indianred', 'black','black','darkred', 'black','black','darkred', 'black','black','blue', 'blue', 'blue']\n",
    "#time_points =  pd.to_datetime([ '1970-01-01 00:12:23', '1970-01-01 00:22:00', '1970-01-01 00:46:05', '1970-01-01 00:48:00']) #  '1970-01-01 00:22:00', '1970-01-01 00:46:05' , '1970-01-01 00:14:00',  '1970-01-01 00:44:02',\n",
    "#labels = ['Wash', 'BK', 'Capsaicin serosa', 'Capsaicin injection', ]\n",
    "\n",
    "\n",
    "# 20_07_2023_stimDrugs\n",
    "#time_points =  pd.to_datetime(['1970-01-01 00:10:00', '1970-01-01 00:11:00', '1970-01-01 00:15:00', '1970-01-01 00:17:00', '1970-01-01 00:20:30', '1970-01-01 00:21:30', '1970-01-01 00:24:04', '1970-01-01 00:27:00', '1970-01-01 00:29:40', '1970-01-01 00:31:00', '1970-01-01 00:34:00', '1970-01-01 00:36:00', '1970-01-01 00:38:00', '1970-01-01 00:43:00', '1970-01-01 00:45:04'])\n",
    "#labels = ['ASP1', 'ASP2', 'Wash', 'BK', 'Wash', 'Wash', 'BK', 'Wash', 'Wash', 'BK', 'BK','Wash', 'Wash', 'Capsaicin srosa', 'Capsaicin injection']\n",
    "#colors = ['darkblue', 'cornflowerblue', 'black', 'darkred', 'black','black','darkred', 'black','black','darkred','indianred', 'black','black', 'blue', 'blue']\n",
    "#time_points =  pd.to_datetime([  '1970-01-01 00:23:00', '1970-01-01 00:38:00',  '1970-01-01 00:45:02', '1970-01-01 00:47:00']) #1970-01-01 00:24:04'\n",
    "#labels = [ 'BK', 'Wash','Capsaicin serosa', 'Capsaicin injection']\n",
    "\n",
    "#time_points =  pd.to_datetime(['1970-01-01 00:26:00']) #, '1970-01-01 00:35:00'])\n",
    "#labels = ['Contraction1'] #, 'Contraction2']\n",
    "\n",
    "# 20_07_2023_stimDrugs2\n",
    "#time_points =  pd.to_datetime(['1970-01-01 00:01:10','1970-01-01 00:03:05','1970-01-01 00:06:00','1970-01-01 00:08:30','1970-01-01 00:10:00', '1970-01-01 00:12:00', '1970-01-01 00:15:04', '1970-01-01 00:17:00', '1970-01-01 00:19:02', '1970-01-01 00:22:00', '1970-01-01 00:24:00', '1970-01-01 00:26:00', '1970-01-01 00:28:00', '1970-01-01 00:30:00', '1970-01-01 00:32:00', '1970-01-01 00:34:00', '1970-01-01 00:36:00', '1970-01-01 00:39:00', '1970-01-01 00:41:07', '1970-01-01 00:43:30'])\n",
    "#labels = ['Wash','ASP', 'contraction', 'Wash', 'BK', 'BK2', 'Wash', 'Wash', 'BK3', 'BK4', 'Wash', 'BK5','Wash', 'Wash', 'BK6', 'Wash', 'Wash', 'Capsaicin serosa', 'Capsaicin injection', 'Capsaicin injection3']\n",
    "#colors = ['black','darkblue', 'black','black','black','darkred', 'indianred', 'black','black','darkred','indianred', 'black','darkred','black','black', 'darkred','black','black', 'blue', 'blue', 'blue']\n",
    "\n",
    "#time_points =  pd.to_datetime(['1970-01-01 00:10:03', '1970-01-01 00:12:04', '1970-01-01 00:19:05', '1970-01-01 00:22:00', '1970-01-01 00:26:04', '1970-01-01 00:32:00', ])\n",
    "#labels = ['BK', 'BK2',  'BK3', 'BK4', 'BK5', 'BK6']\n",
    "\n",
    "#time_points =  pd.to_datetime(['1970-01-01 00:01:07','1970-01-01 00:10:03', '1970-01-01 00:39:03', '1970-01-01 00:43:33'])\n",
    "#labels = ['Wash', 'BK', 'Capsaicin serosa', 'Capsaicin injection']\n",
    "\n",
    "#time_points =  pd.to_datetime(['1970-01-01 00:06:00'])\n",
    "#labels = ['Contraction']\n",
    "\n",
    "'''\n",
    "time_points =  pd.to_datetime(['1970-01-01 00:10:03'])\n",
    "labels = ['BK']\n",
    "'''\n",
    "\n",
    "## Chronic recordings\n",
    "# Day 1 \n",
    "\n",
    "# Chronic_Gut_Rat2A_Rat3B_Day-1_231024_124851_baseline\n",
    "#time_points =  pd.to_datetime(['1970-01-01 00:00:00','1970-01-01 00:05:00'])\n",
    "#labels = ['base_1', 'base_2']\n",
    "#colors = ['cornflowerblue', 'cornflowerblue']\n",
    "\n",
    "# Chronic_Gut_Rat2A_Day-1_231024_132052_baseline+eat \n",
    "#time_points =  pd.to_datetime(['1970-01-01 00:01:30','1970-01-01 00:03:00', '1970-01-01 00:05:45']) #, '1970-01-01 00:07:23'])\n",
    "#labels = ['clean_pre_1', 'clean_pre_2','clean_during'] #,'noise_after_eat']\n",
    "#colors = ['cornflowerblue', 'cornflowerblue', 'green'] #, 'indianred']\n",
    "\n",
    "# Chronic_Gut_Rat2A_Day-1_231024_133626_15 min after eat\n",
    "#time_points =  pd.to_datetime(['1970-01-01 00:00:00','1970-01-01 00:02:00', '1970-01-01 00:05:30', '1970-01-01 00:07:00'])\n",
    "#labels = ['post_1', 'post_2','post_3', 'post_4']\n",
    "#colors = ['purple', 'indianred', 'indianred', 'indianred']\n",
    "\n",
    "#Chronic_Gut_Rat2A_Day-1_231024_134645_eat again\n",
    "#time_points =  pd.to_datetime(['1970-01-01 00:00:50','1970-01-01 00:03:30','1970-01-01 00:05:00','1970-01-01 00:10:00','1970-01-01 00:11:00', '1970-01-01 00:20:00', '1970-01-01 00:30:00', '1970-01-01 00:32:00'])\n",
    "#labels = ['grooming','still','pre_1', 'pre_2', 'during', 'post_2', 'post_3','post_4']\n",
    "#colors = ['black','purple','cornflowerblue', 'cornflowerblue', 'green', 'indianred', 'indianred','indianred','indianred']\n",
    "\n",
    "'''\n",
    "# Day 7\n",
    "# Chronic_Gut_Rat2A_Day-7_231030_124826_good eat\n",
    "time_points =  pd.to_datetime(['1970-01-01 00:06:41','1970-01-01 00:14:00', '1970-01-01 00:15:00','1970-01-01 00:20:20']) #, '1970-01-01 00:07:23'])\n",
    "labels = ['clean_pre_1', 'clean_pre_2','clean_pre_3','clean_during'] #,'noise_after_eat']\n",
    "colors = ['cornflowerblue', 'cornflowerblue','cornflowerblue', 'green'] #, 'indianred']\n",
    "'''\n",
    "'''\n",
    "# Chronic_Gut_Rat2A_Day-7_231030_132351_after eat\n",
    "time_points =  pd.to_datetime(['1970-01-01 00:01:30','1970-01-01 00:03:10']) \n",
    "labels = ['post_1', 'post_2']\n",
    "colors = ['indianred', 'indianred'] \n",
    "'''\n",
    "'''\n",
    "# Chronic_Gut_Rat2A_Day-7_231030_132921_after eat\n",
    "time_points =  pd.to_datetime(['1970-01-01 00:07:30','1970-01-01 00:09:00','1970-01-01 00:10:30','1970-01-01 00:12:00']) \n",
    "labels = ['post_1', 'post_2','post_3', 'post_4']\n",
    "colors = ['indianred', 'indianred','indianred','indianred'] \n",
    "'''\n",
    "\n",
    "# Day 14\n",
    "'''\n",
    "# Chronic_Gut_Rat2A_Day-14_231106_122309_baseline\n",
    "time_points =  pd.to_datetime(['1970-01-01 00:09:56','1970-01-01 00:11:00'])\n",
    "labels = ['base_1', 'base_2'] \n",
    "colors = ['cornflowerblue', 'cornflowerblue']\n",
    "\n",
    "#Chronic_Gut_Rat2A_Day-14_231106_125501_baseline\n",
    "time_points =  pd.to_datetime(['1970-01-01 00:00:25','1970-01-01 00:02:50', '1970-01-01 00:04:20'])\n",
    "labels = ['base_1', 'base_2','base_3'] \n",
    "colors = ['cornflowerblue', 'cornflowerblue','cornflowerblue']\n",
    "'''\n",
    "\n",
    "# Ex vivo \n",
    "# bk 12-10-23\n",
    "#time_points =  pd.to_datetime(['1970-01-01 00:08:30','1970-01-01 01:23:00'])\n",
    "#labels = ['BK1', 'Bk2'] \n",
    "#colors = ['darkred', 'darkred']\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------\n",
    "## Round 2\n",
    "# Rat D1-1\n",
    "#time_points =  pd.to_datetime(['1970-01-01 00:06:30', '1970-01-01 00:08:00', '1970-01-01 00:10:30']) \n",
    "#labels = ['Nif-base', 'Bath nif', 'Nif'] # \n",
    "#colors = ['darkblue', 'green', 'darkred']\n",
    "\n",
    "# Rat D1-2\n",
    "#time_points =  pd.to_datetime(['1970-01-01 00:15:00', '1970-01-01 00:19:00', '1970-01-01 00:21:00', '1970-01-01 00:24:00']) \n",
    "#labels = ['Nif+Atr-base', 'Bath lido', 'Nif+Atr', 'lido'] # \n",
    "#colors = ['darkblue', 'green', 'darkred', 'cornflowerblue']\n",
    "\n",
    "# Rat D2\n",
    "#time_points =  pd.to_datetime(['1970-01-01 00:05:00', '1970-01-01 00:06:00', '1970-01-01 00:10:00', '1970-01-01 00:10:30','1970-01-01 00:11:20', '1970-01-01 00:15:00',  '1970-01-01 00:18:30', '1970-01-01 00:25:00']) \n",
    "#labels = ['Nif-base', 'Nif Bath', 'Nif', 'Nif bath', 'Nif+atr bath', 'Nif+atr','Lido bath', 'Nif+atr'] # \n",
    "#colors = ['darkred', 'indianred', 'darkred', 'indianred', 'cornflowerblue', 'darkblue','black', 'darkblue']\n",
    "\n",
    "# Rat D3\n",
    "#time_points =  pd.to_datetime(['1970-01-01 00:10:00', '1970-01-01 00:11:00', '1970-01-01 00:15:00', '1970-01-01 00:17:20', '1970-01-01 00:18:00',  '1970-01-01 00:25:00']) \n",
    "#labels = ['Nif+Atr-base', 'Bath', 'Nif+Atr', 'Lido bath', 'Defecating', 'Lido dead'] # \n",
    "#colors = ['darkblue', 'green', 'darkred', 'black', 'black', 'black']\n",
    "\n",
    "# Rat D4\n",
    "#time_points =  pd.to_datetime(['1970-01-01 00:01:10','1970-01-01 00:04:00', '1970-01-01 00:04:25', '1970-01-01 00:07:09', '1970-01-01 00:10:00']) \n",
    "#labels = ['Pre', 'Nif+Atr-base', 'Bath',  'Ligate', 'Nif+Atr'] # \n",
    "#colors = ['black', 'darkblue', 'green', 'black', 'chocolate']\n",
    "\n",
    "# Rat D5\n",
    "#time_points =  pd.to_datetime(['1970-01-01 00:02:00', '1970-01-01 00:07:30', '1970-01-01 00:08:30', '1970-01-01 00:12:30', '1970-01-01 00:13:00','1970-01-01 00:14:00']) \n",
    "#labels = ['Contraction?','Nif+Atr-base', 'Bath',  'Ligate', 'Nif+Atr', 'Release'] # \n",
    "#colors = ['green','darkblue', 'green', 'black', 'chocolate', 'black']\n",
    "\n",
    "# Rat D6\n",
    "#time_points =  pd.to_datetime(['1970-01-01 00:05:24', '1970-01-01 00:05:30', '1970-01-01 00:08:00', '1970-01-01 00:11:00', '1970-01-01 00:12:30']) \n",
    "#labels = ['Nif+Atr-base', 'Bath',  'Ligate', 'Nif+Atr', 'Release'] # \n",
    "#colors = ['darkblue', 'green', 'black', 'chocolate', 'black']\n",
    "\n",
    "# Rat D7\n",
    "#time_points =  pd.to_datetime(['1970-01-01 00:04:20','1970-01-01 00:05:00', '1970-01-01 00:05:28', '1970-01-01 00:07:00', '1970-01-01 00:10:00']) \n",
    "#labels = ['Nif+Atr-base', 'Clean', 'Bath',  'Ligate', 'Nif+Atr'] # \n",
    "#colors = ['darkblue', 'black', 'green', 'black', 'chocolate']\n",
    "\n",
    "#Mouse: So it's 2% to 1.5% at 2min, distension at 5min, then 1.5% to 5% iso at 6min with another distension at 8min\n",
    "#time_points =  pd.to_datetime(['1970-01-01 00:02:00', '1970-01-01 00:05:00', '1970-01-01 00:6:00', '1970-01-01 00:8:00']) \n",
    "#labels = ['2->1.5%', 'Dist', '5%', 'Dist']\n",
    "#colors = ['black','darkblue','black', 'darkred']\n",
    "\n",
    "# Pig distension\n",
    "#time_points =  pd.to_datetime(['1970-01-01 00:00:00','1970-01-01 00:01:00', '1970-01-01 00:02:00', '1970-01-01 00:03:00', '1970-01-01 00:04:00', '1970-01-01 00:05:00', '1970-01-01 00:06:00', '1970-01-01 00:07:00', '1970-01-01 00:08:00']) \n",
    "#labels = ['base', 'Tension', 'base', 'torsion','base','Tension', 'base', 'torsion','base']\n",
    "#colors = ['black','purple','black', 'pink', 'black','purple','black', 'pink', 'black']\n",
    "\n",
    "# Pig drugs\n",
    "#time_points =  pd.to_datetime(['1970-01-01 00:00:10','1970-01-01 00:02:00', '1970-01-01 00:12:00', '1970-01-01 00:13:00', '1970-01-01 00:14:00', '1970-01-01 00:24:00']) \n",
    "#labels = ['Base', 'BK', 'Saline', 'Base', 'Caps', 'Saline' ]\n",
    "#colors = ['black','darkblue','green', 'black', 'darkred', 'green']\n",
    "time_points =  pd.to_datetime(['1970-01-01 00:00:10','1970-01-01 00:02:00', '1970-01-01 00:14:00']) \n",
    "labels = ['Baseline (2min)','BK (at 2min)', 'Caps (at 14min)' ]\n",
    "colors = ['black','darkblue','darkred']\n",
    "\n",
    "\n",
    "'''\n",
    "# Drugs repeat 1\n",
    "time_points =  pd.to_datetime(['1970-01-01 00:10:00', '1970-01-01 00:12:00', '1970-01-01 00:15:00', '1970-01-01 00:17:00', '1970-01-01 00:17:30',\n",
    "                                '1970-01-01 00:20:00', '1970-01-01 00:22:00', '1970-01-01 00:25:00', '1970-01-01 00:25:30', '1970-01-01 00:27:00', \n",
    "                                '1970-01-01 00:29:00', '1970-01-01 00:32:00', '1970-01-01 00:34:00', '1970-01-01 00:34:30', '1970-01-01 00:36:00',\n",
    "                                '1970-01-01 00:39:00', '1970-01-01 00:39:30', '1970-01-01 00:41:00', '1970-01-01 00:41:30', '1970-01-01 00:43:00', \n",
    "                                '1970-01-01 00:45:00', '1970-01-01 00:57:00', '1970-01-01 00:59:00' ])\n",
    "labels = ['BK', 'BK2', 'Wash', 'Wash', 'clean',\n",
    "            'BK3', 'BK4', 'Wash', 'clean','Wash',  \n",
    "            'BK5','Wash', 'Wash', 'clean','BK6', \n",
    "            'Wash','clean', 'Wash', 'dry', 'Capsaicin serosa', \n",
    "             'Capsaicin injection', 'caps blocker topical', 'caps + blocker' ]\n",
    "colors = ['darkred', 'indianred', 'black','black','black',\n",
    "            'darkred','indianred', 'black','black','black',\n",
    "            'darkred','black','black','black', 'darkred',\n",
    "            'black','black', 'black','black', 'blue', 'blue', 'black', 'black']\n",
    "'''\n",
    "'''\n",
    "# Drugs repeat 2\n",
    "time_points =  pd.to_datetime(['1970-01-01 00:10:00', '1970-01-01 00:12:00', '1970-01-01 00:15:00', '1970-01-01 00:17:00', '1970-01-01 00:20:00', \n",
    "                                '1970-01-01 00:22:00', '1970-01-01 00:25:00', '1970-01-01 00:27:00', '1970-01-01 00:27:30', \n",
    "                                '1970-01-01 00:29:00', '1970-01-01 00:32:00', '1970-01-01 00:34:00', '1970-01-01 00:44:00', '1970-01-01 00:46:00',\n",
    "                                '1970-01-01 00:46:30','1970-01-01 00:48:00', '1970-01-01 00:50:00', '1970-01-01 00:52:00', '1970-01-01 00:54:00', '1970-01-01 00:56:00'])\n",
    "labels = ['BK', 'BK2', 'Wash', 'Wash', 'BK3', \n",
    "        'BK4', 'Wash', 'Wash', 'clean', \n",
    "        'BK5','Wash', 'Wash', 'BK6', 'Wash',\n",
    "        'dry','nif+BK', 'nif+bk+Atr','Wash','Caps block', 'Capsaicin +blocker']\n",
    "colors = ['darkred', 'indianred', 'black','black','darkred',\n",
    "         'indianred', 'black','black','black',\n",
    "         'darkred','black','black','darkred','black',\n",
    "          'black','darkgreen', 'green','black', 'darkblue', 'blue']\n",
    "'''\n",
    "'''\n",
    "# Drugs repeat 3\n",
    "time_points =  pd.to_datetime(['1970-01-01 00:10:00', '1970-01-01 00:12:00', '1970-01-01 00:15:00', '1970-01-01 00:17:00', '1970-01-01 00:17:30',\n",
    "                                '1970-01-01 00:20:00', '1970-01-01 00:22:00', '1970-01-01 00:25:00', '1970-01-01 00:25:30', '1970-01-01 00:27:00', \n",
    "                                '1970-01-01 00:29:00', '1970-01-01 00:32:00', '1970-01-01 00:34:00', '1970-01-01 00:34:30', '1970-01-01 00:36:00',\n",
    "                                '1970-01-01 00:39:00', '1970-01-01 00:39:30', '1970-01-01 00:41:00', '1970-01-01 00:41:30', '1970-01-01 00:43:00', '1970-01-01 00:45:00'])\n",
    "labels = ['BK', 'BK2', 'Wash', 'Wash', 'clean',\n",
    "            'BK3', 'BK4', 'Wash', 'clean','Wash',  \n",
    "            'BK5','Wash', 'Wash', 'clean','BK6', \n",
    "            'Wash','clean', 'Wash', 'dry', 'Capsaicin serosa', 'Capsaicin injection']\n",
    "colors = ['darkred', 'indianred', 'black','black','black',\n",
    "            'darkred','indianred', 'black','black','black',\n",
    "            'darkred','black','black','black', 'darkred',\n",
    "            'black','black', 'black','black', 'blue', 'blue']\n",
    "'''\n",
    "'''\n",
    "# Drugs repeat 4\n",
    "time_points =  pd.to_datetime(['1970-01-01 00:10:00', '1970-01-01 00:12:00', '1970-01-01 00:15:00', '1970-01-01 00:17:00', '1970-01-01 00:17:30',\n",
    "                                '1970-01-01 00:20:00', '1970-01-01 00:22:00', '1970-01-01 00:25:00', '1970-01-01 00:25:30', '1970-01-01 00:27:00', \n",
    "                                '1970-01-01 00:29:00', '1970-01-01 00:32:00', '1970-01-01 00:34:00', '1970-01-01 00:34:30', '1970-01-01 00:36:00',\n",
    "                                '1970-01-01 00:39:00', '1970-01-01 00:39:30', '1970-01-01 00:41:00', '1970-01-01 00:41:30', '1970-01-01 00:43:00', '1970-01-01 00:45:00'])\n",
    "labels = ['BK', 'BK2', 'Wash', 'Wash', 'clean',\n",
    "            'BK3', 'BK4', 'Wash', 'clean','Wash',  \n",
    "            'BK5','Wash', 'Wash', 'clean','BK6', \n",
    "            'Wash','clean', 'Wash', 'dry', 'Capsaicin serosa', 'Capsaicin injection']\n",
    "colors = ['darkred', 'indianred', 'black','black','black',\n",
    "            'darkred','indianred', 'black','black','black',\n",
    "            'darkred','black','black','black', 'darkred',\n",
    "            'black','black', 'black','black', 'blue', 'blue']\n",
    "'''\n",
    "'''\n",
    "# Drugs repeat 5\n",
    "time_points =  pd.to_datetime(['1970-01-01 00:11:00', '1970-01-01 00:13:00', '1970-01-01 00:15:00', '1970-01-01 00:17:00', '1970-01-01 00:20:00', '1970-01-01 00:22:00', '1970-01-01 00:22:30',\n",
    "                               '1970-01-01 00:25:00', '1970-01-01 00:27:00', '1970-01-01 00:30:00', '1970-01-01 00:30:30', '1970-01-01 00:32:00', \n",
    "                                '1970-01-01 00:34:00', '1970-01-01 00:37:00', '1970-01-01 00:39:00', '1970-01-01 00:39:30', '1970-01-01 00:41:00',\n",
    "                                '1970-01-01 00:44:00', '1970-01-01 00:44:30', '1970-01-01 00:46:00', '1970-01-01 00:46:30', '1970-01-01 00:49:00', '1970-01-01 00:50:00'])\n",
    "labels = ['BK','wash', 'BK1', 'BK2', 'Wash', 'Wash', 'clean',\n",
    "            'BK3', 'BK4', 'Wash', 'clean','Wash',  \n",
    "            'BK5','Wash', 'Wash', 'clean','BK6', \n",
    "            'Wash','clean', 'Wash', 'dry', 'Capsaicin serosa', 'Capsaicin injection']\n",
    "colors = ['darkred','black','darkred', 'indianred', 'black','black','black',\n",
    "            'darkred','indianred', 'black','black','black',\n",
    "            'darkred','black','black','black', 'darkred',\n",
    "            'black','black', 'black','black', 'blue', 'blue']\n",
    "'''\n",
    "\n",
    "# original colors  # uncomment for silencing and drugs Round 1 \n",
    "#colors = ['darkblue', 'cornflowerblue', 'darkred', 'indianred'] # SILENCING/DRUGS\n",
    "#colors = ['#304592',  '#7E8BBB', '#D12C2C', '#E27B7B', '#a000c6', '#0044fb'] #'#a000c6', '#0044fb',  # DRUGS\n",
    "\n",
    "# Define Savitzky-Golay filter parameters\n",
    "window_size = 800\n",
    "order = 2\n",
    "\n",
    "# Create a list to store the extracted periods\n",
    "extracted_periods = []\n",
    "extracted_periods_raw = []\n",
    "\n",
    "# Loop through each time point and extract the corresponding period\n",
    "for time_point in time_points:\n",
    "    # Use boolean indexing to extract the period that matches the time point\n",
    "    period = record.recording.loc[time_point- pd.Timedelta(seconds=seconds_bef) :time_point+ pd.Timedelta(seconds=seconds_after)]\n",
    "    extracted_periods.append(period)        \n",
    "\n",
    "sys.exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### General plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_label = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------\n",
    "# PLOT 1: Plot the extracted periods using the time index (to underlying signal)\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(record.recording.index,record.recording['ch_%s'%ch], '#E5E4E2')\n",
    "for i, period in enumerate(extracted_periods):\n",
    "    plt.plot(period.index, period['ch_%s'%ch], label=labels[i], color=colors[i]) #f\"Period {i+1}\")\n",
    "plt.ylim([-1000,1000])\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Data')\n",
    "plt.title('Extracted Periods')\n",
    "plt.legend(loc='lower center', bbox_to_anchor=(0.5, -0.15))\n",
    "#no_label = False\n",
    "if no_label:\n",
    "    # Get the current axes (equivalent to ax)\n",
    "    ax = plt.gca()\n",
    "    # Set the custom formatter for both the x-axis and y-axis\n",
    "    ax.xaxis.set_major_formatter(FuncFormatter(hide_tick_labels))\n",
    "    ax.yaxis.set_major_formatter(FuncFormatter(hide_tick_labels))\n",
    "    plt.legend('')\n",
    "    \n",
    "\n",
    "plt.savefig('%s/figures/%s_ylim_Fig1_%s_ch%s_extracted-locs-%sseg-%s_%s.tiff' %(record.path,record.recording.name, port,ch, seconds,current_time, no_label), facecolor='w')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------\n",
    "# PLOT 2: Plot all extracted periods superimposed \n",
    "lines = []\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i, period in enumerate(extracted_periods):\n",
    "    if signal2plot == 'smoothed':\n",
    "        # Apply Savitzky-Golay filter for smoothing\n",
    "        smoothed_data = savgol_filter(period['ch_%s'%ch], window_size, order)\n",
    "        line, = plt.plot(period.index - period.index.min(), smoothed_data, label=labels[i], color=colors[i]) #f\"Period {i+1}\")\n",
    "    else:\n",
    "        line, = plt.plot(period.index - period.index.min(), period['ch_%s'%ch], label=labels[i], color=colors[i]) #f\"Period {i+1}\")\n",
    "    lines.append(line)\n",
    "    \n",
    "plt.xlabel('Time (%ss window)'%seconds)\n",
    "plt.ylabel('Data')\n",
    "plt.title('Extracted Periods Superimposed')\n",
    "plt.legend()\n",
    "if no_label:\n",
    "    # Get the current axes (equivalent to ax)\n",
    "    ax = plt.gca()\n",
    "    # Set the custom formatter for both the x-axis and y-axis\n",
    "    ax.xaxis.set_major_formatter(FuncFormatter(hide_tick_labels))\n",
    "    ax.yaxis.set_major_formatter(FuncFormatter(hide_tick_labels))\n",
    "    \n",
    "plt.show()\n",
    "if signal2plot == 'smoothed':\n",
    "    plt.savefig('%s/figures/Fig2_%s_ch%s_superimposed-extracted-smoothed-%s.png' %(record.path, port,ch, current_time), facecolor='w')\n",
    "else:\n",
    "    #plt.savefig('%s/figures/Fig2-%s_ch%s_superimposed-extracted-%sseg-%s_%s.png' %(record.path, port,ch, seconds, current_time, no_label), facecolor='w')\n",
    "    plt.savefig('%s/figures/Fig2-%s_ch%s_superimposed-extracted-%sseg-%s_%s.tiff' %(record.path, port,ch, seconds, current_time, no_label), facecolor='w')\n",
    "\n",
    "# Get the colors and y-axis limits from the superimposed figure\n",
    "colors = [line.get_color() for line in lines]\n",
    "y_min, y_max = plt.gca().get_ylim()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT 3: Plot each extracted period in different rows\n",
    "#'''\n",
    "#no_label = True\n",
    "zoom = False\n",
    "# Calculate the number of rows and columns for the grid\n",
    "num_signals = len(extracted_periods)\n",
    "num_cols = int(math.ceil(math.sqrt(num_signals)))\n",
    "num_rows = int(math.ceil(num_signals / num_cols))\n",
    "\n",
    "# Plot each extracted period in a grid of subplots\n",
    "fig, axs = plt.subplots(num_rows, num_cols, figsize=(12, 8), sharex=True, sharey=True)\n",
    "fig.suptitle('Extracted Periods', fontsize=16)\n",
    "\n",
    "auc = []\n",
    "maxim = []\n",
    "rang = []\n",
    "\n",
    "for i, (period, ax, color) in enumerate(zip(extracted_periods, axs.flatten(),colors)):\n",
    "    ax.plot(period.index - period.index.min(), period['ch_%s'%ch],color=color, label=labels[i]) #'grey'\n",
    "    ax.set_ylabel('Data')\n",
    "    ax.set_title(labels[i])\n",
    "    if zoom:\n",
    "        ax.set_ylim(-2500, 2500) # 2500 Low freque, 200 high freq\n",
    "    else:\n",
    "        ax.set_ylim(y_min, y_max)\n",
    "    if no_label:\n",
    "        # Set the custom formatter for both the x-axis and y-axis\n",
    "        ax.xaxis.set_major_formatter(FuncFormatter(hide_tick_labels))\n",
    "        ax.yaxis.set_major_formatter(FuncFormatter(hide_tick_labels))\n",
    "    \n",
    "    ####### Extract AUC\n",
    "    # Filter the DataFrame to include only rows with positive y-values\n",
    "    positive_period = period['ch_%s'%ch][period['ch_%s'%ch] > 0]\n",
    "    #print(positive_period)\n",
    "\n",
    "    # Calculate the area under the curve using the trapezoidal rule\n",
    "    #x_values = positive_period.index\n",
    "    # Calculate the time difference in hours for each row\n",
    "    x_values = (positive_period.index- pd.Timestamp(\"1970-01-01\")) // pd.Timedelta(nanoseconds=1)\n",
    "    print(x_values.values)\n",
    "    y_values = positive_period.values\n",
    "    print(y_values)\n",
    "    area_under_curve = np.trapz(y_values, x=x_values.values)\n",
    "    auc.append(area_under_curve)\n",
    "\n",
    "    ### Extract max and range\n",
    "    max_val = period['ch_%s'%ch].max()\n",
    "    maxim.append(max_val)\n",
    "    range_val = period['ch_%s'%ch].max() - period['ch_%s'%ch].min()\n",
    "    rang.append(range_val)\n",
    "    \n",
    "print(\"Normalise Area under the curve for positive values (normalised):\", (auc-min(auc))/(max(auc)-min(auc)))\n",
    "print(\" Area under the curve for positive values (AUC-minus min):\", (auc-min(auc)))\n",
    "print(\" ABsolute area under the curve for positive values (AUC):\", (auc))\n",
    "print(\" Max:\", (maxim))\n",
    "print(\" Range:\", (rang))\n",
    "\n",
    "# Remove any empty subplots\n",
    "for i in range(num_signals, num_rows * num_cols):\n",
    "    fig.delaxes(axs.flatten()[i])\n",
    "\n",
    "axs[num_rows - 1, 0].set_xlabel('Time  (%s window)'%seconds)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.savefig('%s/figures/Fig3-%s_%s_ch%s_-extracted-%s_%s.tiff' %(record.path, port,zoom,ch, current_time, no_label), facecolor='w')\n",
    "plt.show()\n",
    "\n",
    "sys.exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Contraction visualisation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PLOT for contraction visualisation\n",
    "from scipy.signal import hilbert, chirp\n",
    "# Function to extract the envelope of a signal\n",
    "def extract_envelope(signal):\n",
    "    analytic_signal = hilbert(signal)\n",
    "    amplitude_envelope = np.abs(analytic_signal)\n",
    "    return amplitude_envelope\n",
    "\n",
    "current_time = now.strftime(\"%d%m%Y_%H%M%S\")\n",
    "cmap = 'gist_ncar'\n",
    "\n",
    "# Define Savitzky-Golay filter parameters\n",
    "window_size = int(record.fs)\n",
    "order = 2\n",
    "\n",
    "lines = []\n",
    "signal_ch = [7,15]\n",
    "no_label = False\n",
    "print(signal_ch)\n",
    "num_signals = len(signal_ch)\n",
    "#print(num_signals)\n",
    "num_cols = int(math.ceil(math.sqrt(num_signals)))\n",
    "num_rows = int(math.ceil(num_signals / num_cols))\n",
    "print(num_cols)\n",
    "print(num_rows)\n",
    "\n",
    "signal2plot = 'envelop' # 'envelop'\n",
    "\n",
    "# Initialize a list to store smoothed envelopes\n",
    "smoothed_envelopes = []\n",
    "\n",
    "#print(extracted_periods)\n",
    "# Create the subplots grid\n",
    "fig, axs = plt.subplots(num_rows, num_cols, figsize=(15, 8), sharex=True, sharey=True)\n",
    "fig.suptitle('Spectrogram filtered signal', fontsize=16)\n",
    "fig2, axs2 = plt.subplots(num_rows, num_cols, figsize=(8, 8), sharex=True, sharey=True)\n",
    "fig3, axs3 = plt.subplots(figsize=(8, 8), sharex=True, sharey=True)\n",
    "period = extracted_periods[0]\n",
    "#print(period)\n",
    "\n",
    "for i,ch,ax,ax2 in zip(range(0,len(signal_ch),1),signal_ch, axs.flat, axs2.flat):\n",
    "    if signal2plot == 'smoothed':\n",
    "        # Apply Savitzky-Golay filter for smoothing\n",
    "        smoothed_data = savgol_filter(period['ch_%s'%ch], window_size, order)\n",
    "        line, = ax2.plot(period.index - period.index.min(), smoothed_data, label='ch: %s'%ch) #, color=colors[i]) #f\"Period {i+1}\")\n",
    "    elif signal2plot == 'envelop':\n",
    "        sp = 'env'\n",
    "        envelope = extract_envelope(period['ch_%s'%ch])\n",
    "        #smoothed_env = savgol_filter(envelop, window_size, order)\n",
    "        smoothed_env = np.convolve(envelope, np.ones(window_size)/window_size, mode='same')\n",
    "        smoothed_envelopes.append(smoothed_env)  # Store the smoothed envelope in the list\n",
    "\n",
    "        line, = ax2.plot(period.index - period.index.min(), smoothed_env, label='ch: %s'%ch, color='grey') #, color=colors[i]) #f\"Period {i+1}\")\n",
    "        ax2.set_ylim([0, 4]) #[-40,40])\n",
    "    else:\n",
    "        sp = 's'\n",
    "        line, = ax2.plot(period.index - period.index.min(), period['ch_%s'%ch], label='ch: %s'%ch, color='grey') #f\"Period {i+1}\")\n",
    "        line, = axs3.plot(period.index - period.index.min(), period['ch_%s'%ch], label='ch: %s'%ch, color='grey') #f\"Period {i+1}\")\n",
    "        ax2.set_ylim([-40, 40]) #[-40,40])\n",
    "    \n",
    "    \n",
    "    # PLOT the spectrogram and store the returned imageAxis\n",
    "    powerSpectrum, freqenciesFound, times_spec, imageAxis = ax.specgram(period['ch_%s' % ch], NFFT=512*8, Fs=record.fs, mode='psd', scale='dB', cmap=cmap) # colorbar_ticks=colorbar_ticks_filt)\n",
    "    \n",
    "    ax.set_ylabel('ch: %s'%ch)\n",
    "    ax.set_xlabel('Time [s]')\n",
    "    # Adjust frequency limits based on your data characteristics\n",
    "    ax.set_ylim([0, filt_config['W'][1]]) #[filt_config['W'][0]\n",
    "    if no_label:\n",
    "        # Set the custom formatter for both the x-axis and y-axis\n",
    "        ax.xaxis.set_major_formatter(FuncFormatter(hide_tick_labels))\n",
    "        ax.yaxis.set_major_formatter(FuncFormatter(hide_tick_labels))\n",
    "        ax2.xaxis.set_major_formatter(FuncFormatter(hide_tick_labels))\n",
    "        ax2.yaxis.set_major_formatter(FuncFormatter(hide_tick_labels))\n",
    "        axs3.xaxis.set_major_formatter(FuncFormatter(hide_tick_labels))\n",
    "        axs3.yaxis.set_major_formatter(FuncFormatter(hide_tick_labels))\n",
    "        ax.set_ylabel('')\n",
    "        ax.set_xlabel('')\n",
    "    \n",
    "    # Adjust the color scale normalization to highlight changes\n",
    "    norm = plt.Normalize(vmin=-40, vmax = 20) #colorbar_ticks_filt[-1], vmax=colorbar_ticks_filt[0]) # 0, -60 -40,20\n",
    "    imageAxis.set_norm(norm)\n",
    "    # Create a colorbar associated with the current subplot\n",
    "    clb = plt.colorbar(imageAxis, ax=ax)\n",
    "    clb.ax.set_title('10*np.log10 \\n [dB/Hz]')\n",
    "    \n",
    "# Calculate the standard deviation of the smoothed envelopes\n",
    "if smoothed_envelopes:\n",
    "    average_envelope = np.mean(smoothed_envelopes, axis=0)\n",
    "    #std_envelope = np.std(smoothed_envelopes, axis=0)\n",
    "\n",
    "    # Calculate upper and lower bounds for shading\n",
    "    #upper_bound = average_envelope + std_envelope\n",
    "    #lower_bound = average_envelope - std_envelope\n",
    "    plt.figure()\n",
    "    # Plot the average smoothed envelope\n",
    "    line, = plt.plot(period.index - period.index.min(), average_envelope, label='Average Envelope', color='grey')\n",
    "    plt.ylim([0, 4]) #[-40,40])\n",
    "    plt.savefig('%s/figures/Contraction_%s_%s_average_%s_%s_ch%s-%sseg-%s.svg' %(record.path, sp,filt_config['W'], no_label, port,ch, seconds, current_time), facecolor='w')\n",
    "    # Shade the area between upper and lower bounds to represent standard deviation\n",
    "    #plt.fill_between(period.index - period.index.min(), lower_bound, upper_bound, color='lightblue', alpha=0.3)\n",
    "\n",
    "    \n",
    "plt.xlabel('Time (%ss window)'%seconds)\n",
    "plt.ylabel('Data')\n",
    "fig2.suptitle('Extracted Periods')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.savefig('%s/figures/Contraction_%s_%s_average_%s_%s_ch%s-%sseg-%s.svg' %(record.path, sp,filt_config['W'], no_label, port,ch, seconds, current_time), facecolor='w')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Drug plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------\n",
    "# PLOT 5:  Plot array of filtered spectrograms - drug recordings\n",
    "# PLOT 6:  Rolling AUC - drug recordings\n",
    "# PLOT 7:  Plot array of PSD filtered signal \n",
    "#---------------------------------------------------------\n",
    "from scipy import integrate #import simps  # For numerical integration\n",
    "cmap = 'gist_ncar' # 'nipy_spectral'\n",
    "\n",
    "no_label = False\n",
    "#Calculate the number of rows and columns for the grid\n",
    "num_signals = len(extracted_periods)\n",
    "#num_cols = int(math.ceil(math.sqrt(num_signals)))\n",
    "#num_rows = int(math.ceil(num_signals / num_cols))\n",
    "num_cols = 2  # Set the number of columns to 4\n",
    "num_rows = int(math.ceil(num_signals / num_cols))\n",
    "colorbar_ticks = [0, -60]\n",
    "times = [4, 30, 50, 99] #sec within spectrum to extract PSD\n",
    "nperseg = 512\n",
    "\n",
    "leng = 2603 # from max number of samples\n",
    "\n",
    "# Create the subplots grid\n",
    "fig, axs = plt.subplots(num_rows, num_cols, figsize=(18, 8), sharex=True) #, sharey=True)\n",
    "fig.suptitle('Spectrogram filtered signal', fontsize=16)\n",
    "\n",
    "fig2, axs2 = plt.subplots(1, len(labels), figsize=(16, 4), sharex=True, sharey=True) #len(times)\n",
    "fig2.suptitle('Drugs PSD at specific times %s' %times, fontsize=16)\n",
    "fig3, axs3 = plt.subplots(len(labels), 1, figsize=(8, 4), sharex=True, sharey=True)\n",
    "fig3.suptitle('Area under the curve in a rolling window', fontsize=16)\n",
    "#fig4, axs4 = plt.subplots(1, len(times), figsize=(12, 4), sharex=True, sharey=True)\n",
    "#fig4.suptitle('Caps Inj PSD at specific times', fontsize=16)\n",
    "\n",
    "# Define the rolling window size and step size (adjust as needed)\n",
    "window_size = int(1*leng/(seconds_after+seconds_bef))  # Number of columns (time steps) in the rolling window\n",
    "print(window_size)\n",
    "step_size = 1   # Number of columns to shift the window each time\n",
    "\n",
    "# Create an empty DataFrame to store the AUC values in separate columns\n",
    "AUC_roll = pd.DataFrame()\n",
    "    \n",
    "# Iterate through the extracted_periods and axs\n",
    "for period, ax, ax3, label in zip(extracted_periods, axs.flat, axs3.flat, labels):\n",
    "    x_seconds = (period.index - period.index.min()).total_seconds()\n",
    "    \n",
    "    # PLOT the spectrogram and store the returned imageAxis\n",
    "    powerSpectrum, freqenciesFound, times_spec, imageAxis = ax.specgram(period['ch_%s' % ch], NFFT=nperseg, Fs=record.fs, mode='psd', scale='dB', cmap=cmap) # colorbar_ticks=colorbar_ticks_filt)\n",
    "\n",
    "    # -----------------------\n",
    "    # Initialize an empty list to store the area under the curve for each window\n",
    "    areas = []\n",
    "\n",
    "    # Iterate through the powerSpectrum columns with the rolling window\n",
    "    for i in range(0, powerSpectrum.shape[1] - window_size + 1, step_size):\n",
    "        \n",
    "        window = powerSpectrum[:, i:i+window_size]  # Extract the window\n",
    "        frequency_axis = freqenciesFound  # Replace with your actual frequency axis\n",
    "        # Compute the area under the curve using numerical integration\n",
    "        area = np.mean(integrate.simpson(abs(window), dx=(frequency_axis[1] - frequency_axis[0]), axis=0))\n",
    "        #print('----')\n",
    "        #print(area)\n",
    "        areas.append(area)\n",
    "        #print(areas)\n",
    "\n",
    "    # Create a new column with a unique name for each iteration and store the area values\n",
    "    #column_name = label  # Create a unique column name\n",
    "    AUC_roll['%s'%label] = areas\n",
    "    \n",
    "    print('------------%s-------)'%label)\n",
    "    AUC_roll_max = AUC_roll['%s'%label].iloc[int(15*leng/(seconds_after+seconds_bef)):int(60*leng/(seconds_after+seconds_bef))]#-1]    # To second 60 for BK recording, for all responses put back -1\n",
    "    print('AUC_max_value & value: %s & %s' %(np.argmax(AUC_roll_max), np.max(AUC_roll_max)))\n",
    "    print('mean_value: %s' %(np.mean(AUC_roll_max)))\n",
    "    print('var_value: %s' %(np.var(AUC_roll_max)))\n",
    "    \n",
    "    #sys.exit()\n",
    "    # PLOT the cumulative area under the curve\n",
    "    ax3.plot(np.linspace(0, int(len(areas)*(seconds_after+seconds_bef)/leng),len(areas)), areas)\n",
    "    ax3.set_ylim(0, 100) #00)\n",
    "    ax3.set_xlabel('Time')\n",
    "    ax3.set_ylabel('AUC')\n",
    "    \n",
    "    if no_label:\n",
    "        # Set the custom formatter for both the x-axis and y-axis\n",
    "        ax3.xaxis.set_major_formatter(FuncFormatter(hide_tick_labels))\n",
    "        ax3.yaxis.set_major_formatter(FuncFormatter(hide_tick_labels))\n",
    "        ax3.set_title('')\n",
    "    \n",
    "    fig3.savefig('%s/figures/Fig6_%s_%s_ch%s_-AUC-%sseg-%s.png' %(record.path, no_label, port,ch, seconds, current_time), facecolor='w')\n",
    "    fig3.savefig('%s/figures/Fig6_%s_%s_ch%s_-AUC-%sseg-%s.tiff' %(record.path, no_label, port,ch, seconds, current_time), facecolor='w')\n",
    "    fig3.savefig('%s/figures/Fig6_%s_%s_ch%s_-AUC-%sseg-%s.svg' %(record.path, no_label, port,ch, seconds, current_time), facecolor='w')\n",
    "    # -----------------------\n",
    "    \n",
    "    # Get the size of each dimension\n",
    "    rows, columns = powerSpectrum.shape\n",
    "    \n",
    "    # Compute sample corresponding the each time point from times and total number of samples. OJO moved to below on 26th Nov (after running all images) to make it autmatic using columns\n",
    "    samples = [int(t*columns/(seconds_after+seconds_bef)) for t in times] #times for PSD plots  257 (if nperseg=512)/ 129 if nperseg=256 coresponds to time 130s, make proportion\n",
    "    print(samples)\n",
    "    \n",
    "    # PLOT PSD at each time point\n",
    "    if label == 'contraction':\n",
    "        axs2[0].plot(freqenciesFound, powerSpectrum[:,samples[0]])\n",
    "        axs2[1].plot(freqenciesFound, powerSpectrum[:,column_with_max_value])\n",
    "        axs2[2].plot(freqenciesFound, powerSpectrum[:,samples[-1]])\n",
    "        axs2[0].set_xscale('log')   \n",
    "        \n",
    "        fig2.savefig('%s/figures/Fig7_%s_ch%s_-PSD_filtered-%sseg-%s.png' %(record.path, port,ch, seconds, current_time), facecolor='w')\n",
    "        fig2.savefig('%s/figures/Fig7_%s_ch%s_-PSD_filtered-%sseg-%s.tiff' %(record.path, port,ch, seconds, current_time), facecolor='w')\n",
    "        fig2.savefig('%s/figures/Fig7_%s_ch%s_-PSD_filtered-%sseg-%s.svg' %(record.path, port,ch, seconds, current_time), facecolor='w')\n",
    "\n",
    "        \n",
    "    if label == 'BK':\n",
    "        #axs2[0][0].plot(freqenciesFound, (powerSpectrum[:,samples[0]]), label='%s'%times[0], color='darkgreen')\n",
    "        #axs2[0][1].plot(freqenciesFound, (powerSpectrum[:,samples[1]]), label='%s'%times[1], color='mediumseagreen')\n",
    "        #axs2[0][2].plot(freqenciesFound, (powerSpectrum[:,samples[2]]), label='%s'%times[2],color='lightgreen')\n",
    "        #axs2[0][0].set_xscale('log')  \n",
    "        \n",
    "        axs2[0].plot(freqenciesFound, powerSpectrum[:,samples[0]], label='%s'%times[0], color='tan')\n",
    "        axs2[0].plot(freqenciesFound, powerSpectrum[:,samples[1]], label='%s'%times[1], color='darkolivegreen') # column_with_max_value\n",
    "        axs2[0].plot(freqenciesFound, powerSpectrum[:,samples[2]],label='%s'%times[2], color='yellowgreen')\n",
    "        axs2[0].plot(freqenciesFound, powerSpectrum[:,samples[3]],label='%s'%times[3], color='lightgreen')\n",
    "        axs2[0].set_xscale('log')   \n",
    "        axs2[0].legend()\n",
    "        axs2[0].set_ylabel('BK - PSD dB') #10*np.log10(Pxx) = \n",
    "        \n",
    "    if label == 'Capsaicin serosa':        \n",
    "        axs2[1].plot(freqenciesFound, powerSpectrum[:,samples[0]], label='%s'%times[0], color='tan')\n",
    "        axs2[1].plot(freqenciesFound, powerSpectrum[:,samples[1]], label='%s'%times[1], color='darkolivegreen') # column_with_max_value\n",
    "        axs2[1].plot(freqenciesFound, powerSpectrum[:,samples[2]],label='%s'%times[2], color='yellowgreen')\n",
    "        axs2[1].plot(freqenciesFound, powerSpectrum[:,samples[3]],label='%s'%times[3], color='lightgreen')\n",
    "        axs2[1].set_xscale('log')   \n",
    "        axs2[1].legend()\n",
    "        axs2[1].set_ylabel('CAP S - PSD dB') #10*np.log10(Pxx) = \n",
    "\n",
    "        #fig3.savefig('%s/figures/Fig7_capS%s_ch%s_-PSD_filtered-%sseg-%s.png' %(record.path, port,ch, seconds, current_time), facecolor='w')\n",
    "        #fig3.savefig('%s/figures/Fig7_capS%s_ch%s_-PSD_filtered-%sseg-%s.svg' %(record.path, port,ch, seconds, current_time), facecolor='w')\n",
    "    \n",
    "    if label == 'Capsaicin injection':\n",
    "        axs2[2].plot(freqenciesFound, powerSpectrum[:,samples[0]], label='%s'%times[0], color='tan')\n",
    "        axs2[2].plot(freqenciesFound, powerSpectrum[:,samples[1]], label='%s'%times[1], color='darkolivegreen') # column_with_max_value\n",
    "        axs2[2].plot(freqenciesFound, powerSpectrum[:,samples[2]],label='%s'%times[2], color='yellowgreen')\n",
    "        axs2[2].plot(freqenciesFound, powerSpectrum[:,samples[3]],label='%s'%times[3], color='lightgreen')\n",
    "        axs2[2].set_xscale('log')  \n",
    "        #axs2[2].set_yscale('log')  \n",
    "        axs2[2].legend()\n",
    "        axs2[2].set_ylabel('CAP I - PSD dB') #10*np.log10(Pxx) = \n",
    "        axs2[2].set_ylim([0,0.045]) #10\n",
    "        axs2[2].set_xlim([200,2000])\n",
    "        #fig4.savefig('%s/figures/Fig7_capI%s_ch%s_-PSD_filtered-%sseg-%s.png' %(record.path, port,ch, seconds, current_time), facecolor='w')\n",
    "        #fig4.savefig('%s/figures/Fig7_capI%s_ch%s_-PSD_filtered-%sseg-%s.svg' %(record.path, port,ch, seconds, current_time), facecolor='w')\n",
    "    if label == 'Wash':\n",
    "        axs2[3].plot(freqenciesFound, powerSpectrum[:,samples[0]], label='%s'%times[0], color='tan')\n",
    "        axs2[3].plot(freqenciesFound, powerSpectrum[:,samples[1]], label='%s'%times[1], color='darkolivegreen') # column_with_max_value\n",
    "        axs2[3].plot(freqenciesFound, powerSpectrum[:,samples[2]],label='%s'%times[2], color='yellowgreen')\n",
    "        axs2[3].plot(freqenciesFound, powerSpectrum[:,samples[3]],label='%s'%times[3], color='lightgreen')\n",
    "        axs2[3].set_xscale('log')  \n",
    "        axs2[3].legend()\n",
    "        axs2[3].set_ylabel(' Saline - PSD dB') #10*np.log10(Pxx) = \n",
    "\n",
    "\n",
    "    if no_label:\n",
    "        # Set the custom formatter for both the x-axis and y-axis\n",
    "        axs2[0].xaxis.set_major_formatter(FuncFormatter(hide_tick_labels))\n",
    "        axs2[0].yaxis.set_major_formatter(FuncFormatter(hide_tick_labels))\n",
    "        axs2[1].xaxis.set_major_formatter(FuncFormatter(hide_tick_labels))\n",
    "        axs2[1].yaxis.set_major_formatter(FuncFormatter(hide_tick_labels))\n",
    "        axs2[2].xaxis.set_major_formatter(FuncFormatter(hide_tick_labels))\n",
    "        axs2[2].yaxis.set_major_formatter(FuncFormatter(hide_tick_labels))\n",
    "        axs2[3].xaxis.set_major_formatter(FuncFormatter(hide_tick_labels))\n",
    "        axs2[3].yaxis.set_major_formatter(FuncFormatter(hide_tick_labels))\n",
    "        #axs2.set_title('')\n",
    "    fig2.savefig('%s/figures/Fig7_%s_%s_ch%s_-PSD_filtered-%sseg-%s.png' %(record.path, no_label, port,ch, seconds, current_time), facecolor='w')\n",
    "    fig2.savefig('%s/figures/Fig7_%s_%s_ch%s_-PSD_filtered-%sseg-%s.svg' %(record.path,no_label,  port,ch, seconds, current_time), facecolor='w')\n",
    "    \n",
    "    # Adjust the color scale normalization to highlight changes\n",
    "    norm = plt.Normalize(vmin=-50, vmax = -10) #vmax = 20 for round 2 drugs #colorbar_ticks_filt[-1], vmax=colorbar_ticks_filt[0]) # 0, -60\n",
    "    imageAxis.set_norm(norm)\n",
    "    \n",
    "    ax.set_ylabel(label)\n",
    "    ax.set_xlabel('Time [s]')\n",
    "    # Adjust frequency limits based on your data characteristics\n",
    "    ax.set_ylim([0, freq_max])\n",
    "    \n",
    "    if no_label:\n",
    "        # Set the custom formatter for both the x-axis and y-axis\n",
    "        ax.xaxis.set_major_formatter(FuncFormatter(hide_tick_labels))\n",
    "        ax.yaxis.set_major_formatter(FuncFormatter(hide_tick_labels))\n",
    "        ax.set_title('')\n",
    "    \n",
    "    # Create a colorbar associated with the current subplot\n",
    "    clb = plt.colorbar(imageAxis, ax=ax)\n",
    "    clb.ax.set_title('10*np.log10 \\n [dB/Hz]')\n",
    "    #clb.set_ticks(colorbar_ticks_filt)  # Adjust the ticks for the colorbar\n",
    "    \n",
    "#fig.savefig('%s/figures/Fig5 _%s_%s_ch%s_-spectrogram_filtered-%sseg-%s.png' %(record.path,no_label, port,ch, seconds, current_time), facecolor='w')\n",
    "fig.savefig('%s/figures/Fig5_%s_%s_ch%s_-spectrogram_filtered-%sseg-%s.tiff' %(record.path, no_label, port,ch, seconds, current_time), facecolor='w')\n",
    "fig.savefig('%s/figures/Fig5_%s_%s_ch%s_-spectrogram_filtered-%sseg-%s.svg' %(record.path, no_label, port,ch, seconds, current_time), facecolor='w')\n",
    "\n",
    "# Save the DataFrame to a CSV file in the specified folder\n",
    "AUC_roll.to_csv('%s/AUC_roll_%s.csv'%(record.path, filt_config['W']), index=False)  # Set index=False to exclude the index column\n",
    "#print(AUC_roll)\n",
    "\n",
    "# Adjust layout and display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Adjust layout and display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "sys.exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Boxplots & statistic analysis of AUC silencing recordings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## BOXPLOTS of all AUC mechanical distension recordings\n",
    "path = dir_name + ''\n",
    "\n",
    "data_norm = {\n",
    "    'Dist1': [1, 1, 1, 0.06],\n",
    "    'Dist2': [0.79, 0.11, 0.68, 1],\n",
    "    'Dist3': [0.68, 0.01, 0.23, 0.03],\n",
    "    'Dist4': [0, 0, 0, 0]\n",
    "}\n",
    "\n",
    "data = {\n",
    "    'Dist1': [3.42E+11, 8.15E+11, 1.50E+11, 6.10E+10],\n",
    "    'Dist2': [2.68E+11, 9.69E+10, 1.01E+11, 9.88E+11],\n",
    "    'Dist3': [2.33E+11, 1.18E+10, 3.48E+10, 3.59E+10],\n",
    "    'Dist4': [0.00E+00, 0.00E+00, 0.00E+00, 0.00E+00]\n",
    "}\n",
    "\n",
    "data_norm_mean ={\n",
    "    'Dist_pre_mean': [0.895, 0.555, 0.84, 0.53],\n",
    "    'Dist_post_mean': [0.34, 0.005, 0.115, 0.015],\n",
    "}\n",
    "data_mean = {\n",
    "    'Dist_pre_mean': [3.05188E+11, 4.55872E+11, 1.25411E+11, 5.24589E+11],\n",
    "    'Dist_post_mean': [1.16323E+11, 5878040550, 17384168950, 17974769550],\n",
    "}\n",
    "\n",
    "\n",
    "no_label = False\n",
    "\n",
    "datasets = [data_norm, data, data_norm_mean, data_mean]\n",
    "dataset_name = ['data_norm', 'data-min', 'data_norm_mean', 'data_mean']\n",
    "\n",
    "datasets_mean = [data_norm_mean, data_mean]\n",
    "dataset_name_mean = ['data_norm_mean', 'data_mean']\n",
    "colors_mean = [colors[1], colors[3]]\n",
    "\n",
    "# Create a figure and axis\n",
    "fig1, axs = plt.subplots(1,2,figsize=(10, 4))\n",
    "\n",
    "for ax, data, name in zip(axs.flatten(), datasets, dataset_name):\n",
    "    df = pd.DataFrame(data)\n",
    "    sns.set(style=\"whitegrid\")  # Optional: Customize the style\n",
    "\n",
    "    # Boxplot\n",
    "    sns.boxplot(data=df, ax=ax, showfliers=True, palette=colors)  # Set showfliers to True if you want to display outliers\n",
    "\n",
    "    # Dots\n",
    "    sns.stripplot(data=df,ax=ax, color='black', size=8, jitter=True, alpha=0.7)\n",
    "    if no_label:\n",
    "        # Set the custom formatter for both the x-axis and y-axis\n",
    "        ax.xaxis.set_major_formatter(FuncFormatter(hide_tick_labels))\n",
    "        ax.yaxis.set_major_formatter(FuncFormatter(hide_tick_labels))\n",
    "        ax.set_title('')\n",
    "        \n",
    "# Create a figure and axis\n",
    "fig2, axs = plt.subplots(1,2,figsize=(10, 4))\n",
    "\n",
    "for ax, data, name in zip(axs.flatten(), datasets_mean, dataset_name_mean):\n",
    "    df = pd.DataFrame(data)\n",
    "    sns.set(style=\"whitegrid\")  # Optional: Customize the style\n",
    "\n",
    "    # Boxplot\n",
    "    sns.boxplot(data=df, ax=ax, showfliers=True, palette=colors_mean)  # Set showfliers to True if you want to display outliers\n",
    "\n",
    "    # Dots\n",
    "    sns.stripplot(data=df,ax=ax, color='black', size=8, jitter=True, alpha=0.7)\n",
    "\n",
    "    if no_label:\n",
    "        # Set the custom formatter for both the x-axis and y-axis\n",
    "        ax.xaxis.set_major_formatter(FuncFormatter(hide_tick_labels))\n",
    "        ax.yaxis.set_major_formatter(FuncFormatter(hide_tick_labels))\n",
    "        ax.set_title('')\n",
    "        \n",
    "#plt.title('Boxplot with Individual Dots for Silencing Groups')\n",
    "\n",
    "fig1.savefig('%s/distension/AUC_all_comparison_%s.svg' %(path, no_label), facecolor='w')\n",
    "fig2.savefig('%s/distension/AUC_pre-post_comparison_%s.svg' %(path, no_label), facecolor='w')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Distension stst test\n",
    "distpre = [1, 1, 1, 0.06, 0.79, 0.11, 0.68, 1]\n",
    "distpost = [0.68, 0.01, 0.23, 0.03, 0, 0, 0, 0]\n",
    "\n",
    "# Perform an independent two-sample t-test\n",
    "t_stat, p_value = stats.ttest_ind(distpre, distpost)\n",
    "\n",
    "# Print the results\n",
    "print(f'T-statistic: {t_stat}')\n",
    "print(f'P-value: {p_value}')\n",
    "\n",
    "# Determine the significance level (alpha)\n",
    "alpha = 0.05\n",
    "\n",
    "# Compare p-value to alpha\n",
    "if p_value < alpha:\n",
    "    print(\"Reject the null hypothesis. There is a significant difference between the means.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis. There is no significant difference between the means.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Boxplots metrics AUC desensitization + statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BK evolution - desensitization expanded to all datasets (round 2)\n",
    "data = {\n",
    "    'Dataset1': [\n",
    "        [1,  0.7325,     0.5725,       0.0525],  # Max\n",
    "        [1,  0.81372549, 0.5,          0.029411765],  # Mean\n",
    "        [1,  0.977900552, 0.812154696, 0.011049724],  # Var\n",
    "    ],\n",
    "    'Dataset2': [ #DR3\n",
    "    [0.792149099, 0, 0.002111918, 0.001364807],  # Max \n",
    "    [0.791443167, 0, 0.002256279, 0.001532567],  # Mean\n",
    "    [0.629132665, 0, 4.8667e-06, 2.10015e-06],  # Var \n",
    "    ],\n",
    "    'Dataset3': [\n",
    "    [1,  0.00754394,   0.091809181, 0],  # Max\n",
    "    [1,  0.008122801,  0.090693956, 0],  # Mean \n",
    "    [1,  5.97107e-05, 0.00846952, 0],  # Var \n",
    "    ],\n",
    "    'Dataset4': [\n",
    "    [1,  0.807011901, 0.222257961, 0.240913477],  # Max \n",
    "    [1,  0.676056338, 0.225352113, 0.154929577],  # Mean\n",
    "    [1,  0.66457529,  0.055501931, 0.065637066]]  # Var \n",
    "}\n",
    "data = data\n",
    "\n",
    "# BK groups\n",
    "bk_groups = ['BK1', 'BK2', 'BK3', 'BK4'] #\n",
    "\n",
    "# Convert the data into the desired structure\n",
    "result = {}\n",
    "for dataset_name, values in data.items():\n",
    "    result[dataset_name] = {\n",
    "        bk: {'Max': values[0][i], 'Mean': values[1][i], 'Var': values[2][i]}\n",
    "        for i, bk in enumerate(bk_groups)\n",
    "    }\n",
    "\n",
    "# Extract metrics across all datasets\n",
    "metrics = ['Max', 'Mean', 'Var']\n",
    "\n",
    "# Prepare storage for mean and std calculations\n",
    "metric_data = {metric: {bk: [] for bk in bk_groups} for metric in metrics}\n",
    "\n",
    "# Populate the storage\n",
    "for dataset in result.values():\n",
    "    for bk in bk_groups:\n",
    "        for metric in metrics:\n",
    "            metric_data[metric][bk].append(dataset[bk][metric])\n",
    "\n",
    "# Calculate mean and std for each metric across BK groups\n",
    "metric_means = {metric: [] for metric in metrics}\n",
    "metric_stds = {metric: [] for metric in metrics}\n",
    "\n",
    "for metric in metrics:\n",
    "    for bk in bk_groups:\n",
    "        metric_means[metric].append(np.mean(metric_data[metric][bk]))\n",
    "        metric_stds[metric].append(np.std(metric_data[metric][bk]))\n",
    "\n",
    "# Plotting\n",
    "fig, axs = plt.subplots(3, 1, figsize=(10, 12))\n",
    "colors = sns.color_palette('flare', len(bk_groups))\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    # Create the bar plot\n",
    "    axs[i].bar(bk_groups, metric_means[metric], color=colors, alpha=0.7, label='Mean')\n",
    "    \n",
    "    # Prepare data for swarmplot\n",
    "    # Flatten data into a DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'BK': np.repeat(bk_groups, len(data)),\n",
    "        'Metric Value': np.concatenate([metric_data[metric][bk] for bk in bk_groups]),\n",
    "        'Metric': metric,\n",
    "        'Dataset': np.tile(list(data.keys()), len(bk_groups))\n",
    "    })\n",
    "    \n",
    "    # Use swarmplot with the correct data\n",
    "    sns.swarmplot(x='BK', y='Metric Value', data=df, ax=axs[i], color='black', size=8, label='Data Points')\n",
    "\n",
    "    # Asymmetric error bars: no lower error, only upper std deviation\n",
    "    lower_error = np.zeros_like(metric_stds[metric])\n",
    "    upper_error = metric_stds[metric]\n",
    "    axs[i].errorbar(bk_groups, metric_means[metric], yerr=[lower_error, upper_error], fmt='none', \n",
    "                    ecolor='black', capsize=5, label='Upper Std Dev')\n",
    "    \n",
    "    axs[i].set_title(f'{metric} Mean + Upper Std Dev across Datasets')\n",
    "    axs[i].set_ylabel(f'{metric}')\n",
    "    axs[i].set_xlabel('BK Groups')\n",
    "    axs[i].legend()\n",
    "\n",
    "# Adjust layout and display\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "fig.savefig(f'{path}/BK_desensitization_all_{no_label}.svg', facecolor='w')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import levene, kruskal, mannwhitneyu\n",
    "\n",
    "# Perform Levene's test for homogeneity of variances across BK groups\n",
    "levene_results = {}\n",
    "for metric in metrics:\n",
    "    # Collect data from all groups for the current metric\n",
    "    groups = [metric_data[metric][bk] for bk in bk_groups]\n",
    "    \n",
    "    # Apply Levene's test\n",
    "    stat, p_value = levene(*groups)\n",
    "    levene_results[metric] = (stat, p_value)\n",
    "    \n",
    "    # Interpret the results\n",
    "    print(f\"Levene's Test for {metric}:\")\n",
    "    print(f\"Test Statistic = {stat:.4f}, p-value = {p_value:.4f}\")\n",
    "    if p_value < 0.05:\n",
    "        print(f\"Result: Variances across BK groups are significantly different for {metric} (p < 0.05), suggesting non-homogeneity of variances.\")\n",
    "    else:\n",
    "        print(f\"Result: No significant difference in variances across BK groups for {metric} (p >= 0.05), suggesting homogeneity of variances.\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "# Perform appropriate non-parametric tests based on Levene's test results\n",
    "for metric in metrics:\n",
    "    print(f\"Non-Parametric Tests for {metric}:\")\n",
    "\n",
    "    if levene_results[metric][1] >= 0.05:\n",
    "        # Homogeneity of variances holds: Use Kruskal-Wallis test\n",
    "        groups = [metric_data[metric][bk] for bk in bk_groups]\n",
    "        stat, p_value = kruskal(*groups)\n",
    "        print(f\"Kruskal-Wallis Test: H-statistic = {stat:.4f}, p-value = {p_value:.4f}\")\n",
    "        \n",
    "        if p_value < 0.05:\n",
    "            print(\"Result: Significant difference between at least one pair of groups (p < 0.05).\")\n",
    "        else:\n",
    "            print(\"Result: No significant difference between groups (p >= 0.05).\")\n",
    "        \n",
    "    else:\n",
    "        # Non-homogeneity of variances: Use pairwise Mann-Whitney U tests\n",
    "        for i in range(len(bk_groups)):\n",
    "            for j in range(i + 1, len(bk_groups)):\n",
    "                group1 = metric_data[metric][bk_groups[i]]\n",
    "                group2 = metric_data[metric][bk_groups[j]]\n",
    "                stat, p_value = mannwhitneyu(group1, group2, alternative='two-sided')\n",
    "                print(f\"Mann-Whitney U Test {bk_groups[i]} vs {bk_groups[j]}: U-statistic = {stat}, p-value = {p_value:.4f}\")\n",
    "    \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "from scipy.stats import mannwhitneyu\n",
    "from scipy.stats import shapiro, levene\n",
    "\n",
    "# Right now only takes the last dataset, i.e. variance\n",
    "\n",
    "# Perform t-tests between each pair of columns\n",
    "columns = df.columns\n",
    "\n",
    "for i in range(len(columns)):\n",
    "    for j in range(i+1, len(columns)):\n",
    "        col1 = columns[i]\n",
    "        col2 = columns[j]\n",
    "\n",
    "        # Normality test\n",
    "        normal1 = shapiro(df[col1])[1] > 0.05\n",
    "        normal2 = shapiro(df[col2])[1] > 0.05\n",
    "        \n",
    "        # Homogeneity of variance test (Levene's test)\n",
    "        var_test = levene(df[col1], df[col2])[1] > 0.05\n",
    "        \n",
    "        if var_test:\n",
    "            print(\"The variances are homogeneous.\")\n",
    "        else:\n",
    "            print(\"The variances are not homogeneous.\")\n",
    "        \n",
    "        if normal1 and normal2 and var_test:  # Proceed with t-test if both are normally distributed and homogeneous variance\n",
    "            t_stat, p_value = ttest_ind(df[col1], df[col2])\n",
    "            test_type = \"t-test\"\n",
    "        else:\n",
    "            t_stat, p_value = mannwhitneyu(df[col1], df[col2])\n",
    "            test_type = \"Mann-Whitney U test\"\n",
    "\n",
    "        print(f\"{test_type} between {col1} and {col2}:\")\n",
    "        print(f\"Statistic: {t_stat}, p-value: {p_value}\")\n",
    "        \n",
    "        if p_value < 0.05:\n",
    "            print(\"The difference is statistically significant.\")\n",
    "        else:\n",
    "            print(\"The difference is not statistically significant.\")\n",
    "        \n",
    "        \n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Data\n",
    "data_max_300 = {\n",
    "    'Norm_wash': [0, 0, 0, 0, 0, 0.2842],\n",
    "    'Norm_BK': [0.596385542, 0.333333333, 0.355555556, 1, 1, 1],\n",
    "    'norm_capS': [0.486445783, 1, 0.024603175, 0.01189, 0.02083, 0],\n",
    "    'norm_capI': [1, 1, 1, 0.002244, 0.001408, 0.228773]\n",
    "}\n",
    "\n",
    "data_mean_300 = {\n",
    "    'Norm_wash': [0, 0.190283401, 0, 0, 0, 0.525],\n",
    "    'Norm_BK': [0.466424082, 0, 0.467158672, 1, 1, 1],\n",
    "    'norm_capS': [0.489579888, 0.939271255, 0.039483395, 0.0197, 0.02207, 0],\n",
    "    'norm_capI': [1, 1, 1, 0.003174, 0.036144, 0.2875],\n",
    "}\n",
    "\n",
    "data_var_300 = {\n",
    "    'Norm_wash': [0, 0, 0, 0, 0, 0.0718],\n",
    "    'Norm_BK': [0.33, 0.075, 0.2, 1, 1, 1],\n",
    "    'norm_capS': [0.21, 1, 0.0025, 0.000187, 0.00047, 0],\n",
    "    'norm_capI': [0.998, 1, 1, 0.0000537, 0.000042, 0.0574]\n",
    "}\n",
    "\n",
    "datasets = {'Max': data_max_300, 'Mean': data_mean_300, 'Var': data_var_300}\n",
    "conditions = ['Norm_wash', 'Norm_BK', 'norm_capS', 'norm_capI']\n",
    "\n",
    "# Prepare data for plotting\n",
    "plot_data = []\n",
    "\n",
    "for metric, data in datasets.items():\n",
    "    df = pd.DataFrame(data)\n",
    "    df['Metric'] = metric\n",
    "    df = df.reset_index().melt(id_vars=['Metric'], var_name='Condition', value_name='Value')\n",
    "    plot_data.append(df)\n",
    "\n",
    "plot_df = pd.concat(plot_data)\n",
    "\n",
    "# Create a figure and axis\n",
    "fig, axs = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axs = axs.flatten()\n",
    "\n",
    "# Plot each condition\n",
    "for idx, condition in enumerate(conditions):\n",
    "    ax = axs[idx]\n",
    "    condition_data = plot_df[plot_df['Condition'] == condition]\n",
    "    \n",
    "    # Check if condition_data is empty\n",
    "    if condition_data.empty:\n",
    "        continue\n",
    "    \n",
    "    sns.barplot(x='Metric', y='Value', hue='Metric', data=condition_data, ax=ax, palette='viridis')\n",
    "    sns.swarmplot(x='Metric', y='Value', data=condition_data, ax=ax, color='black', size=8, alpha=0.7)\n",
    "    \n",
    "    ax.set_title(condition)\n",
    "    ax.set_xlabel('Metric')\n",
    "    ax.set_ylabel('Normalized Values')\n",
    "\n",
    "# Adjust layout and display\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig('%s/AUC_metrics_metricPerCondition_%s.svg' %(path, no_label), facecolor='w')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment if desired to continue with the filtered signal for analysis\n",
    "record.recording=record.filtered\n",
    "record.recording.name = 'filtered'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Fast ICA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Run first time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "record.all_ch_list = [col for col in record.original.columns if col.startswith('ch_')]\n",
    "#[all_ch_list.remove(i) for i in ['ch_14']] #, 'ch_5', 'ch_16', 'ch_25']]\n",
    "n_components = len(record.all_ch_list)\n",
    "print(n_components)\n",
    "# Extract numbers using regular expression\n",
    "record.new_channels = [int(re.search(r'\\d+', item).group()) for item in record.all_ch_list]\n",
    "print(record.new_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "run_first_time = True\n",
    "\n",
    "if run_first_time:\n",
    "    signal = record.recording[record.all_ch_list]\n",
    "    minutes = 35\n",
    "    # 1. Run whole dataset\n",
    "    time_start = time.time()\n",
    "    dur = int(record.fs*minutes*60)\n",
    "    ica_result, mixing = record.applyFastICA(signal.head(dur), n_components=n_components, random_state=10)\n",
    "    #ica_result, mixing = record.applyFastICA(signal, n_components=n_components, random_state=10)\n",
    "    print(\"Time elapsed: {} seconds\".format(time.time()-time_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(np.shape(signal))\n",
    "print(np.shape(record.sources_))\n",
    "print(np.shape(mixing))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if run_first_time:\n",
    "    with open('%s/icaTransformer-butter-%smin.pickle'%(path, minutes), 'wb') as f:\n",
    "        pickle.dump(record.icaTransformer, f, pickle.HIGHEST_PROTOCOL)\n",
    "    with open('%s/ica_sources-butter-%smin.pickle'%(path, minutes), 'wb') as f:\n",
    "        pickle.dump(record.sources_, f, pickle.HIGHEST_PROTOCOL)\n",
    "    with open('%s/ica_mixing-butter-%smin.pickle'%(path, minutes), 'wb') as f:\n",
    "        pickle.dump(record.mixing_, f, pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 2. Plot ICA channels in short period of time (5seg)\n",
    "if run_first_time:\n",
    "    dur_select = int(record.fs*120)\n",
    "    time_start = time.time()\n",
    "    record.plot_ica_channels(dur_select,n_components, ylim=None, save_figure=True)\n",
    "\n",
    "    sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def applyInverseFastICA(dur_recording=None):\n",
    "\t\t\"\"\"\n",
    "\t\tCompute inverse ICA:  recover X after removing the noisy components of the ica transform\n",
    "\n",
    "\t\tParameters\n",
    "\t\t------------\n",
    "\t\tdur_recording: \t[int]. Number of samples for the reconstructed dataframe. If None, take only the head of the dataframe\n",
    "\n",
    "\t\tOther used parameters\n",
    "\t\t----------------------\n",
    "\t\tica_ch: \t[list] list with the sources/components to be removed. \n",
    "\t\tch_loc: \t[list of int] list with electrodes locations corresponding to the selected intan channels \n",
    "\t\tmap_array:\t[numpy array] 1D array with the corresponding intan channels for linear electrode device (1-31 electrodes): map_array[0] is intan channel corresponding to electrode 1\n",
    "\n",
    "\t\tReturn\n",
    "\t\t-----------\n",
    "\t\treconstructed_df: [dataframe] with the results of the inverse ICA tranform\n",
    "\n",
    "\t\t\"\"\"\n",
    "\t\tfor i in record.ica_ch:\n",
    "\t\t\trecord.icaTransformer.mixing_[:, i] = np.zeros(np.shape(record.mixing_)[1])\n",
    "\t\treconstructed_ = record.icaTransformer.inverse_transform(record.sources_)\n",
    "\t\tprint(reconstructed_.shape)\n",
    "\n",
    "\t\t# Store results in dataframe\n",
    "\t\treconstructed_df = pd.DataFrame()\n",
    "\t\tprint(len(record.new_channels))\n",
    "\t\tfor i,k in enumerate(record.new_channels): \n",
    "\t\t\t#print(i)\n",
    "\t\t\t#print(k)\n",
    "\t\t\treconstructed_df['ch_%s'%int(record.map_array[k])] = reconstructed_[:, i]\n",
    "\t\t\n",
    "\t\t# Set datetime index\n",
    "\t\tif dur_recording is None:\n",
    "\t\t\treconstructed_df['seconds'] = np.asarray(record.recording['seconds'])\n",
    "\t\telse:\n",
    "\t\t\treconstructed_df['seconds'] = np.asarray(record.recording['seconds'].head(dur_recording))\n",
    "\t\treconstructed_df.index = pd.DatetimeIndex(reconstructed_df.seconds * 1e9)\n",
    "\t\treconstructed_df.index.name = 'time'\n",
    "\t\treconstructed_df.name = 'reconstructed_df'\n",
    "\t\t\n",
    "\t\treturn reconstructed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reconstuct signal only with ICA channels that contain relevant information\n",
    "print(record.map_array[27])\n",
    "if run_first_time:   \n",
    "    keep_ica_ch = [23]\n",
    "    ica_config_text = ['Channels: %s' %keep_ica_ch]\n",
    "    ica_config_text.append('')\n",
    "    \n",
    "    # Remove channels different to keep\n",
    "    record.removeICA(keep_ica_ch)\n",
    "    reconstructed_df = applyInverseFastICA(dur_recording=dur) #record.\n",
    "    \n",
    "    record.recording = reconstructed_df\n",
    "    record.recording.name = 'reconstructed_df'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot reconstructed\n",
    "show_plot = False\n",
    "if run_first_time:\n",
    "    dur_select = int(record.fs*10) #int(record.fs*10)\n",
    "    fig, ax = plt.subplots(32 ,2, figsize=(10,20), sharex=True)\n",
    "    for i,k in enumerate(record.ch_loc): #range(32):\n",
    "        ax[i,0].plot(np.arange(0, dur_select)/record.fs,signal['ch_%s'%int(record.map_array[k])].head(dur_select), label='ch_%s'%int(record.map_array[k]))\n",
    "        ax[i,1].plot(np.arange(0, dur_select)/record.fs,reconstructed_df['ch_%s'%int(record.map_array[k])].head(dur_select))\n",
    "        ax[i,0].legend()\n",
    "        for j in [0,1]:\n",
    "            #ax[i, j].set_ylim([-0.5, 0.5])\n",
    "            # Hide the right and top spines\n",
    "            ax[i, j].spines['right'].set_visible(False)\n",
    "            ax[i, j].spines['top'].set_visible(False)\n",
    "            # Only show ticks on the left and bottom spines\n",
    "            ax[i, j].yaxis.set_ticks_position('left')\n",
    "            ax[i, j].xaxis.set_ticks_position('bottom')    \n",
    "    if show_plot:\n",
    "        plt.show()\n",
    "    if save_figure:\n",
    "        #pass\n",
    "        plt.savefig('%s/figures/ica-reconstructed-%s.png' %(record.path, current_time), facecolor='w')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Save reconstructed into file\n",
    "if run_first_time:\n",
    "    time_start = time.time()\n",
    "    # To avoid loading the data and creating the dataframe every time save it as pickle \n",
    "    print('Saving data into: %s' %(path))\n",
    "    #reconstructed_df.to_csv(r'%s/ica_reconstruction-butter.csv' %(path))\n",
    "    reconstructed_df.to_pickle('%s/ica_reconstruction-butter-%smin.pkl' %(path, minutes))\n",
    "    \n",
    "    # Save configuration parameters\n",
    "    with open('%s/ica_config-at%s.txt' %(path, current_time), 'w+') as f:\n",
    "        f.write('\\n'.join(ica_config_text))\n",
    "    print(\"Time elapsed: {} seconds\".format(time.time()-time_start))\n",
    "    \n",
    "    record.recording = reconstructed_df\n",
    "    record.recording.name = 'reconstructed_df'\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Run following times"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true,
    "tags": []
   },
   "source": [
    "#run_first_time = False\n",
    "if ~run_first_time:\n",
    "    with open('%s/icaTransformer.pickle'%path, 'rb') as f:\n",
    "        record.icaTransformer = pickle.load(f)\n",
    "        #new_icaTransformer = pickle.load(f)\n",
    "    with open('%s/ica_sources.pickle'%path, 'rb') as f:\n",
    "        record.sources_ = pickle.load(f)\n",
    "        #new_sources_ = pickle.load(f)\n",
    "    with open('%s/ica_mixing.pickle'%path, 'rb') as f:\n",
    "        record.mixing_ = pickle.load(f)\n",
    "        #new_mixing_ = pickle.load(f)\n",
    "    \"\"\"\n",
    "    # Comparison check: all good\n",
    "    print(record.icaTransformer.mixing_)\n",
    "    print(new_icaTransformer.mixing_)\n",
    "    print(record.mixing_)\n",
    "    print(new_mixing_)\n",
    "    print(record.sources_)\n",
    "    print(new_sources_)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true,
    "tags": []
   },
   "source": [
    "# Option 1: load previous reconstructed dataset\n",
    "if ~run_first_time:\n",
    "    minutes = 35\n",
    "    time_start = time.time()\n",
    "    # Load from file\n",
    "    reconstructed_df = pd.read_pickle('%s/ica_reconstruction-butter-%smin.pkl' %(path, minutes))\n",
    "    #print(reconstructed_df.head())\n",
    "    print(\"Time elapsed: {} seconds\".format(time.time()-time_start))\n",
    "    \n",
    "    # Change from float64 to float 16\n",
    "    reconstructed_df = convertDfType(reconstructed_df, typeFloat='float32')\n",
    "    \n",
    "    record.recording = reconstructed_df\n",
    "    record.recording.name = 'reconstructed_df'\n",
    "    #sys.exit()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Option 2: get reconstructed dataset from saved mixing matrix and sources\n",
    "if ~run_first_time:\n",
    "    # 2. Plot ICA channels in short period of time (5seg)\n",
    "    dur_select = int(record.fs*10)\n",
    "    time_start = time.time()\n",
    "    record.plot_ica_channels(dur_select,n_components=n_components, ylim=None, save_figure=True)\n",
    "    sys.exit()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Option 2 cont\n",
    "if ~run_first_time:\n",
    "    keep_ch = [17]\n",
    "    ica_config_text = ['Channels: %s' %keep_ch]\n",
    "    config_text.append('')\n",
    "\n",
    "    record.removeICA(keep_ch)\n",
    "    reconstructed_df = record.applyInverseFastICA(dur_recording=dur)\n",
    "\n",
    "    record.recording = reconstructed_df\n",
    "    record.recording.name = 'reconstructed_df'\n",
    "\n",
    "    print(reconstructed_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Plot reconstructed signal\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "signal_ch = [7]\n",
    "plot_ch = signal_ch[0]\n",
    "record.plot_freq_content(reconstructed_df, plot_ch, nperseg=512, max_freq=4000, dtformat='%M:%S.%f',figsize=(10, 10), show=False) \n",
    "if save_figure:\n",
    "    #pass\n",
    "    plt.savefig('%s/figures/ch%s_ICAreconst-%s.png' %(record.path, plot_ch, current_time), facecolor='w')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "tags": []
   },
   "source": [
    "##### Create dataframe with ICA channel (not reconstructed signal) - Not used"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "\n",
    "# Select signal and noise ICA channels\n",
    "signal_ch = 6\n",
    "noise_ch = 'edo' # 17\n",
    "\n",
    "# Build signal dataframe\n",
    "ica_signal=pd.DataFrame(ica_result.T[signal_ch,:] , columns=['ch_ica'])\n",
    "ica_signal['seconds'] = np.asarray(record.filtered['seconds'].head(dur))\n",
    "ica_signal.index = pd.DatetimeIndex(ica_signal.seconds * 1e9)\n",
    "ica_signal.index.name = 'time'\n",
    "ica_signal.name = 'ica'\n",
    "\n",
    "# Build noise dataset\n",
    "#noise = np.sum(np.asarray(ica_result.T))-reconstructed\n",
    "if noise_ch=='edo':\n",
    "    noise_ica = pd.DataFrame(x_e[0:dur], columns=['ch_ica'])\n",
    "else:\n",
    "    noise_ica = pd.DataFrame(ica_result.T[noise_ch,:], columns=['ch_ica'])\n",
    "noise_ica['seconds'] = np.asarray(record.recording['seconds'].head(dur))\n",
    "noise_ica.index = pd.DatetimeIndex(noise_ica.seconds * 1e9)\n",
    "noise_ica.index.name = 'time'\n",
    "noise_ica.name = 'noise_ica'\n",
    "record.recording = ica_signal\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Picard "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "s = np.asarray(signal.T)\n",
    "print(np.shape(s))\n",
    "print(signal.head())\n",
    "print(s[1,:])\n",
    "\n",
    "from picard import picard\n",
    "time_start = time.time()\n",
    "_, _, Y_picard = picard(np.asarray(signal.head(dur).T), ortho=True, random_state=0)\n",
    "print(np.shape(Y_picard))\n",
    "print(\"Time elapsed: {} seconds\".format(time.time()-time_start))\n",
    "\n",
    "\n",
    "print(np.shape(Y_picard))\n",
    "fig, ax = plt.subplots(len(all_ch_list),1, figsize=(10,20), sharex=True)\n",
    "for i, ic in enumerate(Y_picard):\n",
    "    ax[i].plot(np.arange(0, np.shape(Y_picard)[1])/record.fs,ic, label='ica_%s'%i)\n",
    "    ax[i].legend()\n",
    "\n",
    "#reconstructed =  Y_picard[0,:]  #ica_result.T[5,:] +\n",
    "#noise = Y_picard[23,:]\n",
    "ica_signal=pd.DataFrame(reconstructed, columns=['ch_ica'])\n",
    "ica_signal['seconds'] = np.asarray(record.filtered['seconds'].head(dur))\n",
    "ica_signal.index = pd.DatetimeIndex(ica_signal.seconds * 1e9)\n",
    "ica_signal.index.name = 'time'\n",
    "ica_signal.name = 'ica'\n",
    "\n",
    "noise_ica = pd.DataFrame(noise, columns=['ch_ica'])\n",
    "noise_ica['seconds'] = np.asarray(record.filtered['seconds'].head(dur))\n",
    "noise_ica.index = pd.DatetimeIndex(noise_ica.seconds * 1e9)\n",
    "noise_ica.index.name = 'time'\n",
    "noise_ica.name = 'noise_ica'\n",
    "record.recording = ica_signal\n",
    "\n",
    "\n",
    "sys.exit()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Select INTAN channel to analyse \n",
    "signal_ch = [3, 9, 10, 16, 17, 21]\n",
    "plot_ch = signal_ch[0]\n",
    "print(record.map_array)\n",
    "#record.select_channels(signal_ch)\n",
    "print(record.filter_ch)\n",
    "print(record.ch_loc)\n",
    "config_text.append('Channels: %s' %record.channels)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true,
    "tags": []
   },
   "source": [
    "%matplotlib widget\n",
    "time_start = time.time()\n",
    "\n",
    "signal2decompose = record.filtered\n",
    "config_text.append('signal2decompose: %s' %signal2decompose.name)\n",
    "#signal2decompose = noise\n",
    "\n",
    "neural_wvl, neural_wvl_denoised, other_wvl, other_wvl_denoised, substraction_wvl = \\\n",
    "                         record.wavelet_analysis(signal2decompose, neural_wavelet, other_wavelet, \n",
    "                                                plot_ch, ylim=None, dtformat='%M:%S', show_plot=True, verbose=0)\n",
    "neural_wvl.name = 'neural_wvl'\n",
    "try:\n",
    "    neural_wvl_denoised.name = 'neural_wvl_denoised'\n",
    "    other_wvl_denoised.name = 'other_wvl_denoised'\n",
    "except:\n",
    "     print('cannot put name')   \n",
    "other_wvl.name = 'other_wvl'\n",
    "substraction_wvl.name = 'substraction_wvl'\n",
    "print(\"Time elapsed: {} seconds\".format(time.time()-time_start))\n",
    "if save_figure:\n",
    "    pass\n",
    "    #plt.savefig('%s/figures/%s_Wavelet_decomposition-%s.png' %(record.path,port, current_time))\n",
    "#sys.exit()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true,
    "tags": []
   },
   "source": [
    "record.plot_freq_content(other_wvl, int(plot_ch), nperseg=512, max_freq=2000,figsize=(10, 10), show=False) \n",
    "#record.plot_freq_content(other_wvl_denoised, int(plot_ch),nperseg=512, max_freq=2000,figsize=(10, 10), show=False) \n",
    "#record.plot_freq_content(neural_wvl, int(plot_ch), nperseg=512, max_freq=2000,figsize=(10, 10), show=False) \n",
    "#print(other_wvl)\n",
    "#record.plot_signal(other_wvl, plot_ch, record.map_array, record.num_rows,record.num_columns, \n",
    "#                    channels=record.ch_loc,text_label='raw', text_title='Referenced signal', show_plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Wavelet signal normalization "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true,
    "tags": []
   },
   "source": [
    "neural_wvl_norm, other_wvl_norm, substraction_norm = \\\n",
    "                        record.signal_normalization(neural_wvl, other_wvl,plot_ch, ylim=[-10, 10], \n",
    "                                                    figsize=(15, 10), dtformat='%M:%S', show_plot=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "301.997px",
    "width": "729.991px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "320px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
